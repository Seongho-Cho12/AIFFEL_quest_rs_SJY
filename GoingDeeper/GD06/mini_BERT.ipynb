{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b55908b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3487c1d",
   "metadata": {},
   "source": [
    "- vocab_size = 8000\n",
    "- 전체 파라미터 사이즈 : 1M\n",
    "- 10 epochs 학습시킨 모델 만들기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84ceb9b",
   "metadata": {},
   "source": [
    "# 1. Tokenizer 준비\n",
    "- SentencePiece 모델을 이용하여 BERT MLM 학습용 데이터 만들기\n",
    "- vocab_size = 8000 인 sentencepiece 모델 만들기\n",
    "- BERT 에 사용되는 주요 특수문자가 vocab에 포함되어야 한다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c1257ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/models'\n",
    "corpus_file = os.getenv('HOME') + '/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(\"./ko_8000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6bf618f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/bert_pretrain/data/kowiki.txt\n",
      "  input_format: \n",
      "  model_prefix: ko_8000\n",
      "  model_type: BPE\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 999999\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: [SEP]\n",
      "  user_defined_symbols: [CLS]\n",
      "  user_defined_symbols: [MASK]\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: [UNK]\n",
      "  bos_piece: [BOS]\n",
      "  eos_piece: [EOS]\n",
      "  pad_piece: [PAD]\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/bert_pretrain/data/kowiki.txt\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(117) LOG(WARNING) Too many sentences are loaded! (2451287), which may slow down training.\n",
      "trainer_interface.cc(119) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(122) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 2451287 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [PAD]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [UNK]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [BOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [EOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [SEP]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [CLS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [MASK]\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=287452241\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=4411\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 2450254 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 2450254\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 7050692\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1781571 min_freq=424\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=576838 size=20 all=581927 active=38577 piece=▁아\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=390836 size=40 all=591445 active=48095 piece=▁유\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=297873 size=60 all=601378 active=58028 piece=에는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=244712 size=80 all=609974 active=66624 piece=▁성\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=194372 size=100 all=616449 active=73099 piece=까지\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=193674 min_freq=462\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=176838 size=120 all=625299 active=38770 piece=▁우\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=154294 size=140 all=632274 active=45745 piece=▁파\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=140625 size=160 all=639734 active=53205 piece=00\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=125983 size=180 all=645481 active=58952 piece=▁요\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=114855 size=200 all=649839 active=63310 piece=리아\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=114086 min_freq=457\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=106760 size=220 all=657338 active=39316 piece=▁같은\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=100770 size=240 all=662564 active=44542 piece=▁왕\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=96037 size=260 all=670536 active=52514 piece=▁목\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=88392 size=280 all=675441 active=57419 piece=▁f\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=81678 size=300 all=681701 active=63679 piece=▁선수\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=80870 min_freq=446\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=77144 size=320 all=686930 active=39163 piece=▁때문에\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=73218 size=340 all=691000 active=43233 piece=▁조선\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=68829 size=360 all=695717 active=47950 piece=▁천\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=64009 size=380 all=700839 active=53072 piece=▁196\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=60953 size=400 all=706675 active=58908 piece=▁돌\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=60762 min_freq=435\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=58542 size=420 all=711350 active=39712 piece=▁다시\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=55779 size=440 all=715377 active=43739 piece=▁K\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=52528 size=460 all=721839 active=50201 piece=▁모두\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=49784 size=480 all=727356 active=55718 piece=▁히\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=47637 size=500 all=733038 active=61400 piece=▁전쟁\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=47558 min_freq=423\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=45919 size=520 all=738287 active=41703 piece=▁있어\n",
      "bp"
     ]
    }
   ],
   "source": [
    "# vocab_size = 8000인 sentencepiece 모델\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=corpus_file,\n",
    "    model_prefix='ko_8000',\n",
    "    vocab_size=8000,\n",
    "    model_type=\"bpe\",\n",
    "    max_sentence_length=999999,\n",
    "    pad_id=0,\n",
    "    pad_piece=\"[PAD]\",\n",
    "    unk_id=1,\n",
    "    unk_piece=\"[UNK]\",\n",
    "    bos_id=2,\n",
    "    bos_piece=\"[BOS]\",\n",
    "    eos_id=3,\n",
    "    eos_piece=\"[EOS]\",\n",
    "    user_defined_symbols=\"[SEP],[CLS],[MASK]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd90f9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(\"./ko_8000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af707d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁1',\n",
       " '▁이',\n",
       " '으로',\n",
       " '에서',\n",
       " '▁있',\n",
       " '▁2',\n",
       " '▁그',\n",
       " '▁대',\n",
       " '▁사',\n",
       " '이다',\n",
       " '었다',\n",
       " '▁지',\n",
       " '▁수',\n",
       " '▁19',\n",
       " '▁가',\n",
       " '▁시',\n",
       " '▁20',\n",
       " '▁기',\n",
       " '▁전',\n",
       " '▁아',\n",
       " '▁하',\n",
       " '▁있다',\n",
       " '▁다',\n",
       " '▁제',\n",
       " '했다',\n",
       " '하였',\n",
       " '▁일',\n",
       " '▁한',\n",
       " '▁중',\n",
       " '▁정']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):\n",
    "            token = vocab.id_to_piece(id)\n",
    "            vocab_list.append(token)\n",
    "vocab_list[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df7a14bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# [CLS], tokens a, [SEP], token b, [SEP] 형태의 token 생성\n",
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "tokens_org = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "print(tokens_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd350f",
   "metadata": {},
   "source": [
    "# 2. 데이터 전처리 (1) MASK 생성\n",
    "- MLM에 필요한 빈칸(mask)를 전체 토큰의 15%  \n",
    "\n",
    "    -> 그중 80% [MASK}, 10% 랜덤한 토큰, 10%는 원래 토큰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc4c9586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 token의 15% mask\n",
    "mask_cnt = int((len(tokens_org) - 3) * 0.15)\n",
    "mask_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2293267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4] ['▁추', '적', '추', '적']\n",
      "[5, 6] ['▁비', '가']\n",
      "[7, 8] ['▁내', '리는']\n",
      "[9, 10, 11] ['▁날', '이었', '어']\n",
      "[12, 13, 14] ['▁그', '날', '은']\n",
      "[15, 16, 17] ['▁', '왠', '지']\n",
      "[18, 19, 20] ['▁손', '님', '이']\n",
      "[21, 22] ['▁많', '아']\n",
      "[23] ['▁첫']\n",
      "[24, 25] ['▁번', '에']\n",
      "[26, 27] ['▁삼', '십']\n",
      "[28] ['▁전']\n",
      "[29, 30, 31] ['▁둘', '째', '번']\n",
      "[32, 33] ['▁오', '십']\n",
      "[34] ['▁전']\n",
      "[35, 36, 37] ['▁오', '랜', '만에']\n",
      "[38, 39, 40] ['▁받아', '보', '는']\n",
      "[41] ['▁십']\n",
      "[42, 43, 44] ['▁전', '짜', '리']\n",
      "[45, 46, 47] ['▁백', '통', '화']\n",
      "[48, 49, 50] ['▁서', '푼', '에']\n",
      "[52, 53, 54] ['▁손', '바', '닥']\n",
      "[55, 56] ['▁위', '엔']\n",
      "[57, 58, 59] ['▁기', '쁨', '의']\n",
      "[60, 61] ['▁눈', '물이']\n",
      "[62, 63] ['▁흘', '러']\n",
      "[64, 65, 66] ['▁컬', '컬', '한']\n",
      "[67, 68] ['▁목', '에']\n",
      "[69, 70] ['▁모', '주']\n",
      "[71, 72, 73] ['▁한', '잔', '을']\n",
      "[74, 75] ['▁적', '셔']\n",
      "[76] ['▁몇']\n",
      "[77] ['▁달']\n",
      "[78] ['▁포']\n",
      "[79, 80] ['▁전', '부터']\n",
      "[81, 82, 83, 84] ['▁콜', '록', '거', '리는']\n",
      "[85] ['▁아내']\n",
      "[86, 87] ['▁생각', '에']\n",
      "[88, 89, 90] ['▁그', '토', '록']\n",
      "[91, 92] ['▁먹', '고']\n",
      "[93, 94, 95] ['▁싶', '다', '던']\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기 단위로 mask 하기 위한 index 분할\n",
    "cand_idx = []  # word 단위의 index array\n",
    "for (i, token) in enumerate(tokens_org):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "        continue\n",
    "    if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\"는 단어의 시작을 의미하는 값\n",
    "        cand_idx[-1].append(i)\n",
    "    else:\n",
    "        cand_idx.append([i])\n",
    "\n",
    "# 결과확인\n",
    "for cand in cand_idx:\n",
    "    print(cand, [tokens_org[i] for i in cand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44f10737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24, 25],\n",
       " [57, 58, 59],\n",
       " [32, 33],\n",
       " [64, 65, 66],\n",
       " [41],\n",
       " [79, 80],\n",
       " [52, 53, 54],\n",
       " [67, 68],\n",
       " [29, 30, 31],\n",
       " [91, 92],\n",
       " [23],\n",
       " [26, 27],\n",
       " [76],\n",
       " [42, 43, 44],\n",
       " [78],\n",
       " [60, 61],\n",
       " [38, 39, 40],\n",
       " [93, 94, 95],\n",
       " [9, 10, 11],\n",
       " [81, 82, 83, 84],\n",
       " [85],\n",
       " [12, 13, 14],\n",
       " [34],\n",
       " [71, 72, 73],\n",
       " [77],\n",
       " [45, 46, 47],\n",
       " [48, 49, 50],\n",
       " [28],\n",
       " [74, 75],\n",
       " [62, 63],\n",
       " [88, 89, 90],\n",
       " [5, 6],\n",
       " [35, 36, 37],\n",
       " [55, 56],\n",
       " [18, 19, 20],\n",
       " [86, 87],\n",
       " [7, 8],\n",
       " [15, 16, 17],\n",
       " [1, 2, 3, 4],\n",
       " [21, 22],\n",
       " [69, 70]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random mask를 위한 index 섞기\n",
    "random.shuffle(cand_idx)\n",
    "cand_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "654c1533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MASK]', '[MASK]', '[MASK]', '▁삼', '십', '▁전', '▁둘', '째', '번', '[MASK]', '[MASK]', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '작', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '[MASK]', '[MASK]', '[MASK]', '▁눈', '물이', '▁흘', '러', '[MASK]', '[MASK]', '[MASK]', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "mask_lms = []  # mask 된 값\n",
    "for index_set in cand_idx:\n",
    "    if len(mask_lms) >= mask_cnt:  # 현재 mask된 개수가 15%를 넘으면 중지\n",
    "          break\n",
    "    if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "          continue\n",
    "    dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "    for index in index_set:\n",
    "        masked_token = None\n",
    "        if dice < 0.8:  # 80% replace with [MASK]\n",
    "            masked_token = \"[MASK]\"\n",
    "        elif dice < 0.9: # 10% keep original\n",
    "            masked_token = tokens[index]\n",
    "        else:  # 10% random word\n",
    "            masked_token = random.choice(vocab_list)\n",
    "        mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "        tokens[index] = masked_token\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a44db4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_idx   : [23, 24, 25, 32, 33, 41, 57, 58, 59, 64, 65, 66, 79, 80]\n",
      "mask_label : ['▁첫', '▁번', '에', '▁오', '십', '▁십', '▁기', '쁨', '의', '▁컬', '컬', '한', '▁전', '부터']\n"
     ]
    }
   ],
   "source": [
    "# 순서 정렬 및 mask_idx, mask_label 생성\n",
    "mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb047953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_pretrain_mask() : masked LM 을 위한 코퍼스 생성 메소드\n",
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할 (띄어쓰기)\n",
    "    cand_idx = []\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS}\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\" 단어의 시작을 의미\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "\n",
    "    # random mask를 위해서 순서를 섞음 (shuffle)\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출 (sorted 사용)\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d97d7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked LM 을 위한 코퍼스 생성 메소드\n",
    "# Q. 위 코드들을 참고하여 아래 함수를 완성시켜주세요.\n",
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할 (띄어쓰기)\n",
    "    cand_idx = []\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS}\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\" 단어의 시작을 의미\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "\n",
    "    # random mask를 위해서 순서를 섞음 (shuffle)\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출 (sorted 사용)\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2574714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '[MASK]', '▁둘', '째', '번', '▁오', '십', '[MASK]', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '[MASK]', '[MASK]', '[MASK]', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '[MASK]', '[MASK]', '[MASK]', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "mask_idx   : [28, 34, 52, 53, 54, 55, 56, 78, 79, 80, 81, 82, 83, 84]\n",
      "mask_label : ['▁전', '▁전', '▁손', '바', '닥', '▁위', '엔', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens, \"\\n\")\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeea1036",
   "metadata": {},
   "source": [
    "# 3. 데이터 전처리 (2) NSP pair 생성\n",
    "- NSP -> 두 문장이 연속하는지 확인\n",
    "- 2개의 문장을 짝지어 50%확률로 True, Fals를 지정\n",
    "- 두 문장 사이의 segment 처리: 첫번째문장의 segment 0, 두번째 문장은 1, 구분자 [sep]\n",
    "\n",
    "- MLM과 NSP 는 동시에 학습됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da072307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'],\n",
       " ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'],\n",
       " ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\"추적추적 비가 내리는 날이었어\n",
    "그날은 왠지 손님이 많아\n",
    "첫 번에 삼십 전 둘째 번 오십 전\n",
    "오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
    "손바닥 위엔 기쁨의 눈물이 흘러\n",
    "컬컬한 목에 모주 한잔을 적셔\n",
    "몇 달 포 전부터 콜록거리는 아내\n",
    "생각에 그토록 먹고 싶다던\n",
    "설렁탕 한 그릇을 이제는 살 수 있어\n",
    "집으로 돌아가는 길 난 문득 떠올라\n",
    "아내의 목소리가 거칠어만 가는 희박한 숨소리가\n",
    "오늘은 왠지 나가지 말라던 내 옆에 있어 달라던\n",
    "그리도 나가고 싶으면 일찍이라도 들어와 달라던\n",
    "아내의 간절한 목소리가 들려와\n",
    "나를 원망하듯 비는 점점 거세져\n",
    "싸늘히 식어가는 아내가 떠올라 걱정은 더해져\n",
    "난 몰라 오늘은 운수 좋은 날\n",
    "난 맨날 이렇게 살 수 있으면 얼마나 좋을까\"\"\"\n",
    "\n",
    "\n",
    "# 줄 단위로 tokenize\n",
    "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
    "doc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b83f68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이\n",
    "n_test_seq = 64\n",
    "# 최소 길이\n",
    "min_seq = 8\n",
    "# [CLS], tokens_a, [SEB], tokens_b, [SEP]\n",
    "max_seq = n_test_seq - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81c174ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 문장의 최대 길이를 유지하도록 trim 적용 후 50%의 확률로 true/false 케이스를 생성\n",
    "\n",
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eb247cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. 위 코드들을 참고하여 아래 함수를 완성시켜주세요.\n",
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "            current_chunk.append(doc[i])  # line 단위로 추가\n",
    "            current_length += len(doc[i])  # current_chunk의 token 수\n",
    "            if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "#                 print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "                # token a\n",
    "                a_end = 1\n",
    "                if 1 < len(current_chunk):\n",
    "                    a_end = random.randrange(1, len(current_chunk))\n",
    "                tokens_a = []\n",
    "                for j in range(a_end):\n",
    "                    tokens_a.extend(current_chunk[j])\n",
    "                # token b\n",
    "                tokens_b = []\n",
    "                for j in range(a_end, len(current_chunk)):\n",
    "                    tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                #######################################\n",
    "                if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                    is_next = 0     #False\n",
    "                    tokens_t = tokens_a\n",
    "                    tokens_a = tokens_b\n",
    "                    tokens_b = tokens_t\n",
    "                else:\n",
    "                    is_next = 1    #True\n",
    "                # max_seq 보다 큰 경우 길이 조절\n",
    "                trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "                assert 0 < len(tokens_a)\n",
    "                assert 0 < len(tokens_b)\n",
    "\n",
    "#                 print(\"is_next:\", is_next)\n",
    "#                 print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "#                 print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "                #######################################\n",
    "#                 print()\n",
    "\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c7c4954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    max_seq = n_seq - 3\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])\n",
    "        current_length += len(doc[i])\n",
    "\n",
    "        if i == len(doc) - 1 or current_length >= max_seq:\n",
    "            if current_chunk:\n",
    "                # token a/b 분리\n",
    "                a_end = 1\n",
    "                if len(current_chunk) >= 2:\n",
    "                    a_end = random.randrange(1, len(current_chunk))\n",
    "                tokens_a = []\n",
    "                for j in range(a_end):\n",
    "                    tokens_a.extend(current_chunk[j])\n",
    "                tokens_b = []\n",
    "                for j in range(a_end, len(current_chunk)):\n",
    "                    tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                # swap 여부\n",
    "                if random.random() < 0.5:\n",
    "                    is_next = 0\n",
    "                    tokens_a, tokens_b = tokens_b, tokens_a\n",
    "                else:\n",
    "                    is_next = 1\n",
    "\n",
    "                trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "\n",
    "                tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "                segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "                tokens, mask_idx, mask_label = create_pretrain_mask(\n",
    "                    tokens, int(len(tokens) * mask_prob), vocab_list)\n",
    "\n",
    "                input_ids = [vocab.piece_to_id(p) for p in tokens]\n",
    "                attention_mask = [1] * len(input_ids)\n",
    "\n",
    "                instance = {\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"segment\": segment,\n",
    "                    \"is_next\": is_next,\n",
    "                    \"mask_idx\": mask_idx,\n",
    "                    \"mask_label\": mask_label,\n",
    "                    \"attention_mask\": attention_mask\n",
    "                }\n",
    "                instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "    return instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27e5f704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [6, 516, 3740, 4347, 45, 4007, 24, 5524, 3594, 1130, 2574, 3211, 3720, 4, 199, 3650, 3846, 3650, 6, 6, 114, 368, 687, 500, 3626, 6, 6, 6, 3589, 7206, 3603, 516, 4258, 3590, 201, 3623, 6, 254, 3593, 391, 4185, 6, 938, 3975, 254, 64, 4185, 5736, 64, 3974, 2848, 764, 3665, 3595, 2288, 25, 4350, 3615, 456, 3734, 3676, 80, 4802, 4], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [0, 18, 19, 25, 26, 27, 36, 41, 47], 'mask_label': ['[CLS]', '▁비', '가', '▁그', '날', '은', '▁첫', '▁전', '▁전'], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [5, 1623, 6907, 19, 526, 313, 9, 6, 6, 552, 826, 116, 4052, 956, 3904, 3628, 6, 6, 266, 3681, 639, 118, 4217, 3626, 3668, 2884, 1221, 3907, 3605, 2509, 3681, 639, 4, 3315, 4214, 3605, 6, 6, 60, 3639, 34, 4213, 3600, 239, 4282, 851, 290, 119, 6, 6, 1316, 3808, 3731, 368, 2715, 736, 3593, 13, 3777, 3808, 1144, 3602, 2645, 4], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [2, 7, 8, 16, 17, 36, 37, 48, 49], 'mask_label': ['▁살', '▁돌아', '가는', '▁아내', '의', '▁목', '에', '▁전', '부터'], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [6, 58, 3617, 129, 3948, 3598, 4353, 77, 3595, 335, 3810, 118, 3685, 3937, 1298, 4095, 3785, 6, 6, 6, 2715, 3601, 956, 3904, 3628, 3589, 4927, 2160, 220, 3638, 3937, 4, 1613, 3606, 3589, 7206, 3603, 6, 6, 150, 3628, 3774, 114, 2827, 3593, 6, 290, 3628, 3774, 229, 3620, 1689, 3602, 2645, 1663, 33, 4387, 124, 3620, 6, 6, 290, 3628, 4], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [0, 17, 18, 19, 37, 38, 45, 59, 60], 'mask_label': ['[CLS]', '▁식', '어', '가는', '▁나', '가지', '▁있어', '▁들어', '와'], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [5, 826, 6, 6, 2416, 397, 19, 11, 1663, 2580, 3630, 768, 3600, 3797, 4, 826, 1059, 3628, 1613, 3606, 224, 3629, 1812, 6, 4], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [2, 3, 23], 'mask_label': ['▁맨', '날', '▁날'], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb93c9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d1e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b55fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a72acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c9f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cad8ec2",
   "metadata": {},
   "source": [
    "# 4. 데이터 전처리 (3) 데이터셋 완성\n",
    "- BERT pretrain dataset 생성. json으로 저장\n",
    "- 데이터 사이즈가 크므로 np.memmap을 사용. 메모리 사용량 최소화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbebd92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d79f7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0486d2057b55414497ef1a05d6762e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 lines : ['▁지', '미', '▁카', '터']\n",
      "['▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카', '터', '▁주', '니어', '(,', '▁192', '4', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주', '당', '▁출신', '▁미국', '▁3', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '▁~', '▁1981', '년', ')', '이다', '.']\n",
      "['▁그는', '▁2002', '년', '▁말', '▁인', '권', '과', '▁중', '재', '▁역할', '에', '▁대한', '▁공', '로를', '▁인정', '받아', '▁노', '벨', '▁평화', '상을', '▁받', '게', '▁되었다', '.']\n",
      "\n",
      "14 lines : ['▁수학']\n",
      "['▁수학', '(', '數', '學', ',', '▁)', '은', '▁양', ',', '▁구조', ',', '▁공간', ',', '▁변화', ',', '▁미', '적', '분', '▁등의', '▁개념', '을', '▁다루', '는', '▁학', '문', '이다', '.', '▁현대', '▁수학', '은', '▁형식', '▁논', '리를', '▁이용', '해서', '▁공', '리로', '▁구성된', '▁추', '상', '적', '▁구조를', '▁연구', '하는', '▁학', '문', '으로', '▁여겨', '지', '기도', '▁한다', '.', '▁수학', '은', '▁그', '▁구조', '와', '▁발전', '▁과정', '에서는', '▁자연', '과학', '에', '▁속하는', '▁물리', '학을', '▁비롯한', '▁다른', '▁학', '문', '들과', '▁깊', '은', '▁연', '관을', '▁맺', '고', '▁있다', '.', '▁하지만', ',', '▁어느', '▁과학', '의', '▁분야', '들과', '는', '▁달리', ',', '▁자연', '계에서', '▁관측', '되지', '▁않는', '▁개념', '들에', '▁대해서', '까지', '▁이론', '을', '▁일반', '화', '▁및', '▁추', '상', '화', '시', '킬', '▁수', '▁있다는', '▁차', '이가', '▁있다고', '▁한다', '.', '▁수', '학자', '들은', '▁그러', '한', '▁개념', '들에', '▁대해서', '▁추', '측', '을', '▁하고', ',', '▁적', '절', '하게', '▁선택', '된', '▁정의', '와', '▁공', '리', '로부터', '의', '▁엄', '밀', '한', '▁연', '역을', '▁통해', '서', '▁추', '측', '들의', '▁진', '위를', '▁파', '악', '한다', '.']\n",
      "['▁수', '학의', '▁기초', '를', '▁확', '실', '히', '▁세', '우', '기', '▁위해', ',', '▁수', '리', '논', '리', '학과', '▁집합', '론', '이', '▁발전', '하였고', ',', '▁이와', '▁더불어', '▁범', '주', '론', '이', '▁최근', '에도', '▁발전', '되고', '▁있다', '.', '▁“', '근', '본', '▁위', '기', '”', '라는', '▁말', '은', '▁대', '략', '▁19', '00', '년', '에서', '▁1930', '년', '▁사이에', '▁일어난', ',', '▁수', '학의', '▁엄', '밀', '한', '▁기초', '에', '▁대한', '▁탐', '구를', '▁상징', '적으로', '▁보여', '주는', '▁말이다', '.', '▁수', '학의', '▁엄', '밀', '한', '▁기초', '에', '▁대한', '▁몇', '▁가지', '▁의견', '▁불', '일', '치는', '▁오늘날', '에도', '▁계속', '되고', '▁있다', '.', '▁수', '학의', '▁기초', '에', '▁대한', '▁위', '기는', '▁그', '▁당시', '▁수많은', '▁논', '쟁', '에', '▁의해', '▁촉', '발', '되었으며', ',', '▁그', '▁논', '쟁', '에는', '▁칸', '토', '어의', '▁집합', '론', '과', '▁브라', '우', '어', '-', '힐', '베', '르트', '▁논', '쟁', '이', '▁포함', '되었다', '.']\n",
      "\n",
      "4 lines : ['▁수학', '▁상', '수']\n",
      "['▁수학', '에서', '▁상', '수', '란', '▁그', '▁값', '이', '▁변', '하지', '▁않는', '▁불', '변', '량', '으로', ',', '▁변', '수의', '▁반대', '말', '이다', '.', '▁물리', '▁상', '수', '와는', '▁달리', ',', '▁수학', '▁상', '수는', '▁물리', '적', '▁측정', '과는', '▁상', '관', '없이', '▁정의', '된다', '.']\n",
      "['▁특정', '▁수학', '▁상', '수', ',', '▁예를', '▁들', '면', '▁골', '롬', '-', '딕', '맨', '▁상', '수', ',', '▁프랑', '세', '즈', '-', '로', '빈', '슨', '▁상', '수', ',', '▁formula', '_1', ',', '▁레', '비', '▁상', '수', '같은', '▁상', '수는', '▁다른', '▁수학', '상', '수', '▁또는', '▁함수', '와', '▁약', '한', '▁상', '관', '관', '계', '▁또는', '▁강한', '▁상', '관', '관', '계를', '▁갖', '는다', '.']\n",
      "\n",
      "10 lines : ['▁문학']\n",
      "['▁문학', '(', '文', '學', ')', '은', '▁언', '어를', '▁예술', '적', '▁표현', '의', '▁제', '재', '로', '▁삼', '아', '▁새로운', '▁의미', '를', '▁창', '출', '하여', ',', '▁인간', '과', '▁사회', '를', '▁진', '실', '되', '게', '▁묘사', '하는', '▁예술', '의', '▁하', '위', '분', '야', '이다', '.', '▁간', '단', '하게', '▁설명', '하면', ',', '▁언', '어를', '▁통해', '▁인간의', '▁삶', '을', '▁미', '적', '(', '美', '的', ')', '으로', '▁형', '상', '화', '한', '▁것이라고', '▁볼', '▁수', '▁있다', '.', '▁문학', '은', '▁원래', '▁문', '예', '(', '文', '藝', ')', '라고', '▁부', '르는', '▁것이', '▁', '옳', '으며', ',', '▁문', '학을', '▁학', '문', '의', '▁대상', '으로서', '▁탐', '구', '하는', '▁학', '문', '의', '▁명칭', '▁역시', '▁문', '예', '학', '이다', '.', '▁문', '예', '학', '은', '▁음악', '사', '학', ',', '▁미술', '사', '학', '▁등과', '▁함께', '▁예술', '학의', '▁핵', '심', '분', '야', '로서', '▁인', '문', '학의', '▁하', '위', '범', '주에', '▁포함', '된다', '.']\n",
      "['▁반', '영', '론', '적', '▁관', '점에', '▁의한', '▁감', '상은', '▁작품', '을', '▁창', '작', '된', '▁당시', '▁시대', '▁정', '황', '과', '▁연결', '시켜', '▁감', '상', '하는', '▁입', '장', '이고', ',', '▁내', '재', '적', '▁관', '점', '의', '▁감', '상은', '▁작품', '의', '▁형식', ',', '▁내용', '에', '▁국', '한', '하여', '▁감', '상', '하는', '▁것이다', '.', '▁표현', '론', '적', '▁관', '점', '의', '▁감', '상은', '▁작가', '의', '▁전기', '적', '▁사실', '과', '▁작품', '을', '▁연결', '시켜', '▁감', '상', '하는', '▁것이', '고', ',', '▁수용', '론', '적', '▁관', '점', '의', '▁감', '상은', '▁독', '자와', '▁작품', '을', '▁연결', '시켜', '▁감', '상', '하는', '▁것을', '▁말한다', '.']\n",
      "\n",
      "10 lines : ['▁나라', '▁목록']\n",
      "['▁이', '▁문', '서는', '▁나라', '▁목록', '이며', ',', '▁전', '▁세계', '▁20', '6', '개', '▁나라', '의', '▁각', '▁현', '황', '과', '▁주', '권', '▁승', '인', '▁정보를', '▁개', '요', '▁형태로', '▁나', '열', '하고', '▁있다', '.']\n",
      "['▁위', '▁목록', '에', '▁포함', '되지', '▁않은', '▁다음', '▁국가', '는', '▁몬', '테', '비', '데', '오', '▁협', '약', '의', '▁모든', '▁조건', '을', '▁만족', '하지', '▁못', '하거나', ',', '▁자주', '적이고', '▁독립', '적', '임을', '▁주장', '하지', '▁않는', '▁국가', '이다', '.']\n",
      "\n",
      "['▁화학']\n",
      "['▁화학', '(', '化', '學', ',', '▁)', '은', '▁물질', '의', '▁성', '질', ',', '▁조성', ',', '▁구조', ',', '▁변화', '▁및', '▁그', '에', '▁수', '반', '하는', '▁에너', '지의', '▁변', '화를', '▁연구', '하는', '▁자연', '과', '학의', '▁한', '▁분야', '이다', '.', '▁물리', '학', '도', '▁역시', '▁물질', '을', '▁다루', '는', '▁학', '문', '이지만', ',', '▁물리', '학', '이', '▁원', '소', '와', '▁화', '합', '물을', '▁모두', '▁포함한', '▁물', '체의', '▁운동', '과', '▁에너', '지', ',', '▁열', '적', '·', '전', '기', '적', '·', '광', '학적', '·', '기', '계', '적', '▁속', '성을', '▁다루', '고', '▁이러한', '▁현', '상', '으로부터', '▁통일', '된', '▁이론', '을', '▁구축', '하려는', '▁것', '과는', '▁달리', '▁화학', '에서는', '▁물질', '▁자', '체를', '▁연구', '▁대상으로', '▁한다', '.', '▁화학', '은', '▁이미', '▁존재', '하는', '▁물질', '을', '▁이용하여', '▁특', '정한', '▁목', '적', '에', '▁맞', '는', '▁새로운', '▁물질', '을', '▁합', '성', '하는', '▁길', '을', '▁제공', '하며', ',', '▁이는', '▁농', '작', '물의', '▁증', '산', ',', '▁질', '병', '의', '▁치료', '▁및', '▁예', '방', ',', '▁에너', '지', '▁효', '율', '▁증', '대', ',', '▁환경', '오', '염', '▁감소', '▁등', '▁여러', '▁가지', '▁이', '점을', '▁제공', '한다', '.']\n",
      "['▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '해', '낸', '▁화', '합', '물을', '▁뜻', '하였으나', '▁지금', '은', '▁유', '기', '▁화', '합', '물의', '▁범', '위가', '▁크게', '▁넓', '어져', '▁탄', '소', '▁사', '슬', '▁또는', '▁탄', '소', '▁고', '리를', '▁가진', '▁모든', '▁화', '합', '물을', '▁뜻', '한다', '.', '▁유', '기', '화', '학의', '▁오', '랜', '▁관', '심', '사는', '▁유', '기', '▁화', '합', '물의', '▁합', '성', '▁메', '커', '니', '즘', '이다', '.', '▁현', '대에', '▁들어', '서', '▁핵', '자', '기', '▁공', '명', '법', '과', '▁X', '선', '▁결정', '학', '▁등이', '▁개발', '되어', '▁유', '기', '▁화', '합', '물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 위키가 주제별로 잘 나눠지는지 여부 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)  \n",
    "            if 0 < len(doc):\n",
    "                if 0 < count:\n",
    "                    count -= 1\n",
    "                    print(len(doc), \"lines :\", doc[0])\n",
    "                    print(doc[1])\n",
    "                    print(doc[-1])\n",
    "                    print()\n",
    "                else:\n",
    "                    break\n",
    "                doc = []\n",
    "        else:  # 빈 줄이 아니면 doc에 저장\n",
    "            pieces = vocab.encode_as_pieces(line)    \n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        print(doc[0])\n",
    "        print(doc[1])\n",
    "        print(doc[-1])\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bb50392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168336b45b004918bed941b8ad701602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 21 instances: 21\n",
      "{'input_ids': [5, 4, 46, 3607, 3676, 3678, 3606, 812, 2661, 2371, 6, 6, 6, 470, 38, 147, 3637, 16, 3592, 1230, 46, 3607, 268, 3807, 3760, 3606, 3454, 292, 1776, 612, 199, 3764, 3638, 4162, 268, 3807, 1408, 2453, 5590, 1161, 3606, 6, 6, 268, 3807, 2181, 803, 2133, 1153, 1342, 3427, 812, 3681, 6180, 6175, 343, 812, 3681, 70, 402, 1219, 484, 268, 4], 'segment': [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [10, 11, 12, 37, 38, 41, 42, 53, 54], 'mask_label': ['▁화', '합', '물을', '▁뜻', '하였으나', '▁유', '기', '▁사', '슬'], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [5, 4, 46, 3607, 3676, 3678, 3606, 812, 2661, 6, 268, 3807, 1408, 470, 38, 147, 3637, 16, 3592, 1230, 46, 3607, 268, 3807, 3760, 3606, 3454, 292, 1776, 612, 199, 3764, 3638, 4162, 268, 3807, 1408, 420, 1263, 1161, 3606, 6, 6, 268, 3807, 2181, 803, 2133, 1153, 1342, 3427, 812, 3681, 15, 4016, 6, 6, 6, 70, 402, 1219, 484, 6, 4], 'segment': [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [9, 28, 29, 41, 42, 55, 56, 57, 62], 'mask_label': ['▁이루어진', '▁동물', '로부터', '▁유', '기', '▁또는', '▁탄', '소', '▁화'], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "doc: 14 instances: 14\n",
      "{'input_ids': [5, 3592, 110, 867, 403, 3604, 1156, 3624, 3607, 6, 6, 6, 6, 1814, 3661, 817, 3678, 502, 570, 100, 46, 3607, 268, 3807, 3760, 2001, 3593, 6, 818, 1165, 943, 9, 1760, 4045, 184, 3592, 429, 1951, 4399, 3597, 378, 3643, 4059, 3683, 3702, 3594, 70, 3737, 3624, 3760, 3885, 50, 3620, 46, 3607, 3676, 3678, 10, 2556, 665, 3591, 3592, 4, 4], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'is_next': 1, 'mask_idx': [0, 1, 9, 10, 11, 12, 27, 30, 31], 'mask_label': ['[CLS]', '.', '▁공', '명', '법', '과', '▁있어서', '▁방법', '으로'], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [6, 4, 46, 3607, 3676, 3678, 3606, 6, 6, 6, 268, 3807, 1408, 470, 38, 147, 3637, 16, 3592, 1230, 46, 3607, 268, 3807, 3760, 3606, 3454, 292, 6, 6, 199, 3764, 3638, 4162, 268, 3807, 1408, 420, 1263, 1161, 3606, 46, 3607, 268, 3807, 2181, 803, 2133, 6, 1342, 3427, 6, 6, 15, 4016, 343, 812, 3681, 70, 402, 1219, 484, 268, 4], 'segment': [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [0, 7, 8, 9, 28, 29, 48, 51, 52], 'mask_label': ['[CLS]', '▁탄', '소로', '▁이루어진', '▁동물', '로부터', '▁크게', '▁탄', '소'], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "doc: 4 instances: 4\n",
      "{'input_ids': [5, 3592, 110, 867, 403, 3604, 1156, 3624, 3607, 41, 3694, 3789, 3637, 1814, 3661, 817, 3678, 502, 570, 100, 46, 3607, 268, 3807, 3760, 2001, 3593, 6, 818, 1165, 943, 9, 1760, 4045, 184, 3592, 6, 6, 6, 6, 378, 3643, 4059, 3683, 3702, 3594, 70, 3737, 3624, 3760, 3885, 50, 3620, 46, 3607, 3676, 3678, 10, 6, 6, 6, 6, 4, 4], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'is_next': 1, 'mask_idx': [27, 36, 37, 38, 39, 58, 59, 60, 61], 'mask_label': ['▁있어서', '▁플', '라스', '틱', ',', '▁다루', '어진', '다', '.'], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [7716, 4, 46, 3607, 3676, 3678, 3606, 6, 6, 2371, 268, 3807, 1408, 470, 38, 147, 3637, 16, 3592, 6, 46, 3607, 268, 3807, 3760, 3606, 3454, 292, 1776, 612, 199, 3764, 3638, 4162, 268, 3807, 1408, 420, 1263, 6, 6, 46, 3607, 268, 3807, 2181, 803, 2133, 1153, 1342, 3427, 812, 3681, 15, 4016, 343, 812, 3681, 70, 402, 6, 6, 6, 4], 'segment': [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [0, 7, 8, 19, 39, 40, 60, 61, 62], 'mask_label': ['[CLS]', '▁탄', '소로', '▁원래', '▁지금', '은', '▁가진', '▁모든', '▁화'], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "doc: 10 instances: 10\n",
      "{'input_ids': [5, 3592, 110, 867, 6, 6, 1156, 3624, 3607, 41, 3694, 3789, 3637, 1814, 3661, 6, 6, 502, 570, 100, 46, 3607, 268, 3807, 3760, 2001, 3593, 6, 818, 1165, 943, 9, 1760, 4045, 184, 3592, 6, 6, 6, 6, 378, 3643, 4059, 3683, 3702, 3594, 70, 3737, 3624, 3760, 3885, 50, 3620, 46, 3607, 3676, 3678, 10, 2556, 665, 3591, 3592, 4, 4], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'is_next': 1, 'mask_idx': [4, 5, 15, 16, 27, 36, 37, 38, 39], 'mask_label': ['▁들어', '서', '▁결정', '학', '▁있어서', '▁플', '라스', '틱', ','], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [5, 4, 46, 3607, 3676, 3678, 3606, 812, 2661, 2371, 268, 3807, 1408, 6, 6, 147, 3637, 16, 3592, 1230, 6, 6, 268, 3807, 3760, 3606, 3454, 292, 1776, 612, 199, 3764, 3638, 4162, 268, 3807, 1408, 6, 6, 1161, 3606, 46, 3607, 268, 3807, 2181, 803, 2133, 6, 1342, 3427, 812, 3681, 6, 6, 343, 812, 3681, 70, 402, 1219, 484, 268, 4], 'segment': [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [13, 14, 20, 21, 37, 38, 48, 53, 54], 'mask_label': ['▁연구', '하는', '▁유', '기', '▁뜻', '하였으나', '▁크게', '▁사', '슬'], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "doc: 10 instances: 10\n",
      "{'input_ids': [6, 4, 46, 3607, 3676, 3678, 3606, 812, 2661, 6, 268, 3807, 1408, 6, 6, 147, 3637, 16, 3592, 1230, 46, 3607, 268, 3807, 3760, 3606, 3454, 292, 1776, 612, 199, 3764, 3638, 4162, 268, 3807, 1408, 420, 1263, 1161, 3606, 46, 3607, 268, 3807, 2181, 803, 2133, 1153, 1342, 3427, 812, 3681, 6, 6, 6, 812, 3681, 6, 6, 1219, 484, 268, 4], 'segment': [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [0, 9, 13, 14, 53, 54, 55, 58, 59], 'mask_label': ['[CLS]', '▁이루어진', '▁연구', '하는', '▁사', '슬', '▁또는', '▁고', '리를'], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [5, 4, 46, 3607, 3676, 3678, 3606, 812, 2661, 2371, 268, 3807, 1408, 2208, 3548, 147, 3637, 16, 3592, 1230, 46, 3607, 268, 3807, 3760, 3606, 3454, 292, 1776, 612, 199, 3764, 3638, 4162, 6, 6, 6, 420, 1263, 1161, 3606, 6, 6, 268, 3807, 2181, 803, 2133, 1153, 1342, 3427, 812, 3681, 15, 4016, 6, 812, 3681, 70, 402, 6, 484, 268, 4], 'segment': [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [13, 14, 34, 35, 36, 41, 42, 55, 60], 'mask_label': ['▁연구', '하는', '▁화', '합', '물을', '▁유', '기', '▁또는', '▁가진'], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "doc: 31 instances: 31\n",
      "{'input_ids': [5, 3592, 110, 867, 403, 3604, 1156, 3624, 3607, 41, 3694, 3789, 3637, 1814, 3661, 4884, 3295, 502, 570, 100, 46, 3607, 5081, 7046, 5817, 2001, 3593, 1658, 818, 1165, 943, 9, 1760, 4045, 184, 3592, 429, 1951, 4399, 3597, 378, 3643, 4059, 3683, 3702, 3594, 70, 3737, 3624, 3760, 3885, 6, 6, 46, 3607, 3676, 3678, 10, 2556, 665, 3591, 3592, 4, 4], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'is_next': 1, 'mask_idx': [0, 1, 15, 16, 22, 23, 24, 51, 52], 'mask_label': ['[CLS]', '.', '▁결정', '학', '▁화', '합', '물', '▁등', '도'], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [6, 4, 46, 3607, 3676, 3678, 3606, 812, 2661, 2371, 6, 6, 6, 470, 38, 147, 3637, 16, 3592, 1230, 46, 3607, 268, 3807, 3760, 3606, 6, 6, 1776, 612, 199, 3764, 3638, 4162, 268, 3807, 1408, 420, 1263, 1161, 3606, 46, 3607, 268, 3807, 2181, 6, 6, 1153, 1342, 3427, 812, 3681, 15, 4016, 343, 812, 3681, 70, 402, 1219, 6, 268, 4], 'segment': [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [0, 10, 11, 12, 26, 27, 46, 47, 61], 'mask_label': ['[CLS]', '▁화', '합', '물을', '▁식물', '이나', '▁범', '위가', '▁모든'], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instance 생성 기능 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "            if 0 < len(doc):\n",
    "                instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "                # save\n",
    "                print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "                print(instances[0])\n",
    "                print(instances[-1])\n",
    "                print()\n",
    "                doc = []\n",
    "                if 0 < count:  # 테스트를 위해서 부분 처리함\n",
    "                    count -= 1\n",
    "                else:\n",
    "                    break\n",
    "        else:  # doc에 저장\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        instances = create_pretrain_instances(doc, 128)\n",
    "        # save\n",
    "        print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "        print(instances[0])\n",
    "        print(instances[-1])\n",
    "        print()\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59dbda40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT pretrain 데이터셋 생성 메소드\n",
    "# Q. 아래 주석에 따라 코드를 완성해주세요.\n",
    "# Q. 아래 주석에 따라 코드를 완성해주세요.\n",
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):        # 생성되는 단어 목록이 unknown인 경우는 제거합니다. \n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "                     if len(doc) > 0:\n",
    "                         save_pretrain_instances(out_f, doc)\n",
    "                         doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode(line, out_type = str)\n",
    "                    if len(pieces) > 0:\n",
    "                        doc.append(pieces)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ef7c599",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58da38edc3ed48ac946415e43bc62894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# json 포맷으로 저장\n",
    "pretrain_json_path ='./bert_pre_train.json'\n",
    "\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f595eb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json 포맷으로 저장(chunk 단위로 나눠서 처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "788d388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# def make_pretrain_data_batch(sp, corpus_file, output_prefix, seq_len, batch_size=100000):\n",
    "#     with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "#         buffer = []\n",
    "#         file_index = 0\n",
    "#         for i, line in enumerate(f):\n",
    "#             # SentencePiece를 사용한 토크나이징 및 ID 변환\n",
    "#             ids = sp.encode(line.strip(), out_type=int)[:seq_len]\n",
    "#             attention = [1] * len(ids)\n",
    "#             # 패딩 추가 (필요한 경우)\n",
    "#             if len(ids) < seq_len:\n",
    "#                 pad_len = seq_len - len(ids)\n",
    "#                 ids += [sp.pad_id()] * pad_len\n",
    "#                 attention += [0] * pad_len\n",
    "\n",
    "#             data = {\n",
    "#                 'input_ids': ids,\n",
    "#                 'attention_mask': attention\n",
    "#             }\n",
    "#             buffer.append(data)\n",
    "\n",
    "#             if len(buffer) >= batch_size:\n",
    "#                 out_file = f\"{output_prefix}_{file_index:03d}.json\"\n",
    "#                 with open(out_file, 'w', encoding='utf-8') as out_f:\n",
    "#                     json.dump(buffer, out_f, ensure_ascii=False)\n",
    "#                 print(f\"✅ Saved: {out_file}\")\n",
    "#                 buffer = []\n",
    "#                 file_index += 1\n",
    "\n",
    "#         if buffer:\n",
    "#             out_file = f\"{output_prefix}_{file_index:03d}.json\"\n",
    "#             with open(out_file, 'w', encoding='utf-8') as out_f:\n",
    "#                 json.dump(buffer, out_f, ensure_ascii=False)\n",
    "#             print(f\"✅ Saved: {out_file} (last)\")\n",
    "\n",
    "# # 사용 예시\n",
    "# import sentencepiece as spm\n",
    "# sp = spm.SentencePieceProcessor()\n",
    "# sp.load(\"./ko_8000.model\")  # 학습된 모델 경로로 바꿔주세요\n",
    "\n",
    "# make_pretrain_data_batch(sp, corpus_file, './bert_pretrain_chunk', 128, batch_size=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8b9f105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 0,\n",
       " 0,\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# memmap을 사용하여 메모리 사용량 최소화\n",
    "n_seq = 128\n",
    "# [CLS], tokens_a, [SEP], tokens_b, [SEP]\n",
    "max_seq = n_seq - 3\n",
    "\n",
    "# 만약 일반적인 Numpy Array에다 데이터를 로딩한다면 이렇게 되겠지만\n",
    "# enc_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# dec_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# labels_nsp = np.zeros((total,), np.int32)\n",
    "# labels_mlm = np.zeros((total, n_seq), np.int32)\n",
    "\n",
    "# np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "\n",
    "enc_tokens[0], enc_tokens[-1], segments[0], segments[-1], labels_nsp[0], labels_nsp[-1], labels_mlm[0], labels_mlm[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31d8d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "# # 모든 chunk 파일들을 jsonl로 변환\n",
    "# for path in sorted(glob.glob(\"./bert_pretrain_chunk/bert_pretrain_chunk_*.json\")):\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f_in:\n",
    "#         data_list = json.load(f_in)  # list of dicts\n",
    "#     jsonl_path = path.replace(\".json\", \".jsonl\")\n",
    "#     with open(jsonl_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "#         for item in data_list:\n",
    "#             f_out.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "#     print(f\"✅ Converted to JSONL: {jsonl_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4298d692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1095914"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라인수\n",
    "pretrain_json_path = './bert_pre_train.json'\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15589956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "\n",
    "# pretrain_json_path = './bert_pretrain_chunk/'\n",
    "# total = 0\n",
    "\n",
    "# for file_path in glob.glob(os.path.join(pretrain_json_path, \"*.json\")):\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         for _ in f:\n",
    "#             total += 1\n",
    "\n",
    "# print(\"총 라인 수:\", total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d12e6529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 필요한 데이터 로딩 함수 load_pre_train_data()\n",
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = data[\"input_ids\"]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "301a8df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596b934700e74645a24b262f237dca2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62/2100616155.py:43: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_62/2100616155.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_62/2100616155.py:45: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    }
   ],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23a1d8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([   6,   18, 3679,  207, 3707,    6, 1042,  103, 3603, 3679, 3711,\n",
       "          207, 3707,   37, 3418,  416,  810, 3659, 3618,    6,    6,    7,\n",
       "         3622,    6,    6,    6, 1114, 3717,  788,  243,   49, 3625,  796,\n",
       "            6, 1647, 3675, 3675, 3618,    6, 3008, 3618, 3609,   16, 3592,\n",
       "            4,   18, 3679,  207, 3707, 3595, 1755, 3623, 3639,    6,    6,\n",
       "         3565, 3828, 4396,  794, 6077, 5810, 1369,   10, 1605, 3592, 1755,\n",
       "         3623,   41, 3637,  830, 3617, 1135,   52, 3592,   13,    6,   87,\n",
       "         1501, 2247,   25, 3772, 3866, 3660, 3624, 3806, 3866, 4189, 3629,\n",
       "         3772, 3594,  249, 3718, 1232,    6,    6,    6,  479, 3645, 3618,\n",
       "          243, 2780,   14, 1509,  168, 3870,  414,  165, 1697, 4283, 3866,\n",
       "         3696, 3676,  593,   21, 5000,  399, 1927, 3600,  813,   17, 3592,\n",
       "          307,  587,  931,  103, 4306, 4283,    4], dtype=int32),\n",
       " memmap([   5, 3662,  205, 3592,    6,    6,    6,  143, 3655,   19, 3688,\n",
       "         3613,  338, 3708, 4034, 3601,  176, 3593,  742,   84,    6,    6,\n",
       "            6,   33, 1232, 1331,  421,  334,  829,   46, 3688,  165, 3594,\n",
       "          176,    6,  116, 1408, 1069, 1089,  560,   39,    6, 1063, 2931,\n",
       "         1523, 3592,  165,   46, 3680, 3613, 2902, 2693, 2376,  158, 1477,\n",
       "           71, 3676, 1644, 3593, 3181,  577, 3662,  205, 3592,    4, 1899,\n",
       "         3612, 3618,  176,   19, 3688, 3613,  338, 3708, 4034, 3594, 1331,\n",
       "         1232,    6,    6,    6,  339, 3597,   71, 3676,  887, 3593,  365,\n",
       "         3688,   54,  738, 3209, 3597,    6,    6,    6,    6,    6, 3155,\n",
       "          375,   31, 3592,  276,  738, 3688, 3690, 3637,  427, 3643, 3688,\n",
       "         3690, 3594,  369, 3706,    9,  375,   52, 3592,    6,    6,    6,\n",
       "          516, 3879, 4048, 3594,  476, 2788,    4], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 1,\n",
       " 0,\n",
       " memmap([   5,    0,    0,    0,    0, 3324,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,  131, 3655,    0,\n",
       "            0,  203,  241, 3595,    0,    0,    0,    0,    0,    0,    0,\n",
       "          663,    0,    0,    0,    0,  203,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,  630, 3707,\n",
       "            0,    0,  429, 3733, 3621, 3619,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,   81,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,   33,   52, 3592,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0], dtype=int32),\n",
       " memmap([   0,    0,    0,    0, 1899, 3612, 3618,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,   15, 3924,\n",
       "         3730,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0, 3484,    0,    0,    0,    0,    0,    0,  276,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,   29, 3997, 3919,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,  102, 3724, 3663, 3651, 3703,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0, 1034, 3666, 3618,\n",
       "            0,    0,    0,    0,    0,    0,    0], dtype=int32))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처음과 마지막 확인\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e2e2e",
   "metadata": {},
   "source": [
    "# 5. BERT 모델 구현\n",
    "- pad mask, ahead mask 함수\n",
    "- gelu activation 함수\n",
    "- parameter initializer 생성 함수\n",
    "- json을 config 형태로 사용하기 위한 유틸리티 함수  \n",
    "\n",
    "\n",
    "- embedding layer, transformer encoder layer, bert layer 구성\n",
    "- pretrain용 BERT 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57f7d1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# pad mask, ahead mask 함수\n",
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d610f666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# gelu activation 함수\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50f3dc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# parameter initializer 함수\n",
    "\n",
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8de79f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "#json을 config 형태로 사용하기 위한 class\n",
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f69d0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "#Token Embedding \n",
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b46a7d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# positional embedding \n",
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: position embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a89f3f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# segment embedding은 별도의 레이어로 구현하지 않고 bert class에 포함하도록 구성\n",
    "# scaled dot attention\n",
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out, attn_prob\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "573265cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# multihead attention\n",
    "# Q. 주석과 코드를 참조하여 아래 클래스를 완성해주세요.\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out, _ = self.attention(Q_m, K_m, V_m, attn_mask_m)\n",
    "        # transpose and liner\n",
    "        attn_out_m =  tf.transpose(attn_out, perm = [0, 2, 1, 3]) # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c436ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n",
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# transformer encoder layer\n",
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val\n",
    "print(\"슝=3\")\n",
    "\n",
    "    \n",
    "    \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c22366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# bert layer\n",
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d272eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n",
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# pretrain용 모델 구성\n",
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs\n",
    "print(\"슝=3\")\n",
    "    \n",
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
    "    segments = tf.keras.Input(shape=(128,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36260d1",
   "metadata": {},
   "source": [
    "# 6. pretrain 진행\n",
    "- loss, accuracy 함수 정의\n",
    "- learning rate 스케쥴링\n",
    "- 10 epoch 모델 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8b7351d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "353de1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "97cc1818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8b59166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 8000,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7608158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 256), (None, 4483840     input_ids[0][0]                  \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            66304       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 8000)   0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 4,550,144\n",
      "Trainable params: 4,550,144\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc1a9d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "4\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "print(config.d_model)\n",
    "print(config.n_head)\n",
    "print(config.d_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2664cc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 20000\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62f72d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 329s 72ms/step - loss: 19.0922 - nsp_loss: 0.5273 - mlm_loss: 18.5649 - nsp_acc: 0.7536 - mlm_lm_acc: 0.1242\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.12420, saving model to ./bert_pre_train.hdf5\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 289s 72ms/step - loss: 17.2878 - nsp_loss: 0.4900 - mlm_loss: 16.7978 - nsp_acc: 0.7938 - mlm_lm_acc: 0.1441\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.12420 to 0.14413, saving model to ./bert_pre_train.hdf5\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 15.1288 - nsp_loss: 0.4862 - mlm_loss: 14.6425 - nsp_acc: 0.8011 - mlm_lm_acc: 0.1786\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.14413 to 0.17857, saving model to ./bert_pre_train.hdf5\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 14.1209 - nsp_loss: 0.4782 - mlm_loss: 13.6427 - nsp_acc: 0.8132 - mlm_lm_acc: 0.2026\n",
      "\n",
      "Epoch 00004: mlm_lm_acc improved from 0.17857 to 0.20260, saving model to ./bert_pre_train.hdf5\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 13.8037 - nsp_loss: 0.4707 - mlm_loss: 13.3330 - nsp_acc: 0.8247 - mlm_lm_acc: 0.2102\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.20260 to 0.21016, saving model to ./bert_pre_train.hdf5\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 13.7563 - nsp_loss: 0.4693 - mlm_loss: 13.2870 - nsp_acc: 0.8266 - mlm_lm_acc: 0.2116\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.21016 to 0.21157, saving model to ./bert_pre_train.hdf5\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 13.7269 - nsp_loss: 0.4732 - mlm_loss: 13.2537 - nsp_acc: 0.8208 - mlm_lm_acc: 0.2127\n",
      "\n",
      "Epoch 00007: mlm_lm_acc improved from 0.21157 to 0.21267, saving model to ./bert_pre_train.hdf5\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 13.5690 - nsp_loss: 0.4763 - mlm_loss: 13.0927 - nsp_acc: 0.8169 - mlm_lm_acc: 0.2177\n",
      "\n",
      "Epoch 00008: mlm_lm_acc improved from 0.21267 to 0.21767, saving model to ./bert_pre_train.hdf5\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 13.3425 - nsp_loss: 0.4774 - mlm_loss: 12.8651 - nsp_acc: 0.8153 - mlm_lm_acc: 0.2245\n",
      "\n",
      "Epoch 00009: mlm_lm_acc improved from 0.21767 to 0.22451, saving model to ./bert_pre_train.hdf5\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 13.0583 - nsp_loss: 0.4761 - mlm_loss: 12.5822 - nsp_acc: 0.8197 - mlm_lm_acc: 0.2329\n",
      "\n",
      "Epoch 00010: mlm_lm_acc improved from 0.22451 to 0.23287, saving model to ./bert_pre_train.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Q. 모델을 학습시키고, 내용을 history에 담아주세요.\n",
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(\"./bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history = pre_train_model.fit(x = pre_train_inputs, y = pre_train_labels, epochs = 10, batch_size=32, callbacks = [save_weights])\n",
    "# 모델 인자에는 inputs, labels, epochs, batch size, callback 이 필요해요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f92bc75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a04787a",
   "metadata": {},
   "source": [
    "# 7. 결과\n",
    "- 학습 과정 시각화(NSP와 MLM의 loss)\n",
    "- inference model\n",
    "    - masked token 예측\n",
    "    - NSP 판단\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "740b307d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8838a1",
   "metadata": {},
   "source": [
    "- nsp:처음 부터 높고 빠르게 수럼 -> nsp task가 너무 쉬움, nsp 쌍을 더 균형있게 섞는다. \n",
    "- mlm :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703cbb8",
   "metadata": {},
   "source": [
    "### inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "212d9a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nsp_inference_model(pre_train_model):\n",
    "    input_ids = pre_train_model.input[0]\n",
    "    segment_ids = pre_train_model.input[1]\n",
    "    nsp_output = pre_train_model.get_layer('nsp').output\n",
    "\n",
    "    inference_model = tf.keras.Model(inputs=[input_ids, segment_ids], outputs=nsp_output)\n",
    "    return inference_model\n",
    "\n",
    "\n",
    "def build_mlm_inference_model(pre_train_model):\n",
    "    input_ids = pre_train_model.input[0]\n",
    "    segment_ids = pre_train_model.input[1]\n",
    "    mlm_output = pre_train_model.get_layer('mlm').output\n",
    "\n",
    "    inference_model = tf.keras.Model(inputs=[input_ids, segment_ids], outputs=mlm_output)\n",
    "    return inference_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f7054349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 모델 생성 및 가중치 로드\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.load_weights(\"bert_pre_train.hdf5\")\n",
    "\n",
    "# 추론 모델 생성\n",
    "nsp_model = build_nsp_inference_model(pre_train_model)\n",
    "mlm_model = build_mlm_inference_model(pre_train_model)\n",
    "\n",
    "# # 예측 실행\n",
    "# nsp_pred = nsp_model.predict([input_ids, segment_ids])\n",
    "# mlm_pred = mlm_model.predict([input_ids, segment_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "77288207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_nsp_input(text_a, text_b, vocab, max_len=128):\n",
    "    tokens_a = vocab.encode(text_a, out_type=str)\n",
    "    tokens_b = vocab.encode(text_b, out_type=str)\n",
    "\n",
    "    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "    segment_ids = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "    input_ids = [vocab.piece_to_id(t) for t in tokens]\n",
    "\n",
    "    pad_len = max_len - len(input_ids)\n",
    "    input_ids += [vocab.pad_id()] * pad_len\n",
    "    segment_ids += [0] * pad_len\n",
    "\n",
    "    return np.array([input_ids]), np.array([segment_ids])\n",
    "\n",
    "def make_mlm_input(masked_text, vocab, max_len=128):\n",
    "    tokens = vocab.encode(masked_text, out_type=str)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    input_ids = [vocab.piece_to_id(t) for t in tokens]\n",
    "    segment_ids = [0] * len(input_ids)\n",
    "\n",
    "    pad_len = max_len - len(input_ids)\n",
    "    input_ids += [vocab.pad_id()] * pad_len\n",
    "    segment_ids += [0] * pad_len\n",
    "\n",
    "    return np.array([input_ids]), np.array([segment_ids]), tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "020a3114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nsp(text_a, text_b, vocab, model, max_len=128):\n",
    "    # tokenize\n",
    "    tokens_a = vocab.encode(text_a, out_type=str)\n",
    "    tokens_b = vocab.encode(text_b, out_type=str)\n",
    "\n",
    "    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "    segment_ids = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "    input_ids = [vocab.piece_to_id(t) for t in tokens]\n",
    "\n",
    "    # pad\n",
    "    pad_len = max_len - len(input_ids)\n",
    "    input_ids += [vocab.pad_id()] * pad_len\n",
    "    segment_ids += [0] * pad_len\n",
    "\n",
    "    # to array\n",
    "    input_ids = np.array([input_ids])\n",
    "    segment_ids = np.array([segment_ids])\n",
    "\n",
    "    # inference\n",
    "    pred = model.predict([input_ids, segment_ids])\n",
    "    prob = pred[\"nsp\"][0]\n",
    "\n",
    "    print(f\"[NSP] 이어질 확률: {prob[0]:.4f}, 이어지지 않을 확률: {prob[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "57ec9da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSP 예문\n",
    "nsp_pairs = [\n",
    "    (\"세종대왕은 조선의 제4대 왕이다.\", \"한글은 세종대왕이 창제하였다.\"),  #연속\n",
    "    (\"경복궁은 조선 시대의 궁궐 중 하나이다.\", \"아인슈타인은 상대성 이론으로 유명한 물리학자이다.\")  # 비연속\n",
    "]\n",
    "\n",
    "# MLM 예문\n",
    "mlm_sentences = [\n",
    "    \"지미카터는 [MASK] 출신의 대통령이다\",\n",
    "    \"백두산은 [MASK]와 중국의 국경에 위치해 있다.\",\n",
    "    \"지구는 태양계에서 [MASK] 번째 행성이다.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b770e5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 NSP 결과\n",
      "[1] \"세종대왕은 조선의 제4대 왕이다.\" / \"한글은 세종대왕이 창제하였다.\" → [0.73093045 0.26906958] (is_next 확률: 0.2691)\n",
      "[2] \"경복궁은 조선 시대의 궁궐 중 하나이다.\" / \"아인슈타인은 상대성 이론으로 유명한 물리학자이다.\" → [0.27081448 0.72918546] (is_next 확률: 0.7292)\n",
      "\n",
      "🔎 MLM 결과\n",
      "[1] \"지미카터는 [MASK] 출신의 대통령이다\" → [MASK] 예측: \"▁러시아\"\n",
      "[2] \"백두산은 [MASK]와 중국의 국경에 위치해 있다.\" → [MASK] 예측: \"▁km\"\n",
      "[3] \"지구는 태양계에서 [MASK] 번째 행성이다.\" → [MASK] 예측: \"▁세\"\n"
     ]
    }
   ],
   "source": [
    "# NSP 추론\n",
    "print(\"🔎 NSP 결과\")\n",
    "for i, (a, b) in enumerate(nsp_pairs):\n",
    "    input_ids, segment_ids = make_nsp_input(a, b, vocab)\n",
    "    pred = nsp_model.predict([input_ids, segment_ids])[0]\n",
    "    print(f\"[{i+1}] \\\"{a}\\\" / \\\"{b}\\\" → {pred} (is_next 확률: {pred[1]:.4f})\")\n",
    "\n",
    "# MLM 추론\n",
    "print(\"\\n🔎 MLM 결과\")\n",
    "for i, sentence in enumerate(mlm_sentences):\n",
    "    input_ids, segment_ids, tokens = make_mlm_input(sentence, vocab)\n",
    "    pred = mlm_model.predict([input_ids, segment_ids])[0]\n",
    "\n",
    "    masked_idx = tokens.index(\"[MASK]\")\n",
    "    predicted_id = np.argmax(pred[masked_idx])\n",
    "    predicted_token = vocab.id_to_piece(int(predicted_id))\n",
    "\n",
    "    print(f\"[{i+1}] \\\"{sentence}\\\" → [MASK] 예측: \\\"{predicted_token}\\\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "92105592",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_62/681433762.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mvisualize_nsp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"조선은 이성계가 세운 나라이다.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"그는 고려 말 무신 출신이다.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsp_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_62/681433762.py\u001b[0m in \u001b[0;36mvisualize_nsp\u001b[0;34m(text_a, text_b, vocab, model, max_len)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# 모델 예측\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nsp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# 시각화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_nsp(text_a, text_b, vocab, model, max_len=128):\n",
    "    # 기존 test_nsp의 핵심 로직\n",
    "    tokens_a = vocab.encode(text_a, out_type=str)\n",
    "    tokens_b = vocab.encode(text_b, out_type=str)\n",
    "    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "    segment_ids = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "    input_ids = [vocab.piece_to_id(t) for t in tokens]\n",
    "    pad_len = max_len - len(input_ids)\n",
    "    input_ids += [vocab.pad_id()] * pad_len\n",
    "    segment_ids += [0] * pad_len\n",
    "    input_ids = np.array([input_ids])\n",
    "    segment_ids = np.array([segment_ids])\n",
    "\n",
    "    # 모델 예측\n",
    "    pred = model.predict([input_ids, segment_ids])\n",
    "    prob = pred[\"nsp\"][0]\n",
    "\n",
    "    # 시각화\n",
    "    labels = [\"Is Next\", \"Not Next\"]\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.bar(labels, prob, color=[\"green\", \"red\"])\n",
    "    plt.title(\"NSP Prediction\")\n",
    "    plt.ylim(0, 1)\n",
    "    for i, v in enumerate(prob):\n",
    "        plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center', fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "visualize_nsp(\"조선은 이성계가 세운 나라이다.\", \"그는 고려 말 무신 출신이다.\", vocab, nsp_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e46b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mlm(masked_text, vocab, model, max_len=128, top_k=5):\n",
    "    tokens = masked_text.replace(\"[MASK]\", \" [MASK] \").split()\n",
    "    ids = [vocab.piece_to_id(t) if t != \"[MASK]\" else vocab.piece_to_id(\"[MASK]\") for t in tokens]\n",
    "    masked_index = ids.index(vocab.piece_to_id(\"[MASK]\"))\n",
    "\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    input_ids = [vocab.piece_to_id(t) if t != \"[MASK]\" else vocab.piece_to_id(\"[MASK]\") for t in tokens]\n",
    "    segment_ids = [0] * len(input_ids)\n",
    "    pad_len = max_len - len(input_ids)\n",
    "    input_ids += [vocab.pad_id()] * pad_len\n",
    "    segment_ids += [0] * pad_len\n",
    "    input_ids = np.array([input_ids])\n",
    "    segment_ids = np.array([segment_ids])\n",
    "\n",
    "    pred = model.predict([input_ids, segment_ids])\n",
    "    mlm_logits = pred[\"mlm\"][0, masked_index + 1]  # +1 for [CLS]\n",
    "    top = tf.math.top_k(mlm_logits, k=top_k)\n",
    "\n",
    "    tokens = [vocab.id_to_piece(tid.numpy()) for tid in top.indices]\n",
    "    scores = [val.numpy() for val in top.values]\n",
    "\n",
    "    # 시각화\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(tokens, scores, color=\"blue\")\n",
    "    plt.title(\"[MASK] Token Prediction\")\n",
    "    plt.ylabel(\"Score (logit)\")\n",
    "    for i, score in enumerate(scores):\n",
    "        plt.text(i, score + 0.5, f\"{score:.1f}\", ha='center', fontweight='bold')\n",
    "    plt.show()\n",
    "visualize_mlm(\"서울은 대한민국의 [MASK]이다.\", vocab, mlm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab11f304",
   "metadata": {},
   "source": [
    "- NSP 는 is next vs not next 확률을 막대그래프로 표시\n",
    "- mlm은 top-k 예측 단어들과 logit 값을 표시\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e2b90e",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00fa095",
   "metadata": {},
   "source": [
    "# 루브릭\n",
    "- 한글 코퍼스를 가공하여 bert pretrain용 데이터셋을 잘 생성하였다.  \n",
    "    - MLM, NSP task 특징이 잘 반영됨 \n",
    "- 구현한 bert 모델의 학습이 안정적으로 진행됨을 확인\n",
    "    - 학습진행 과정 중에 MLM, NSP loss가 안정적으로 감소\n",
    "- 1M 짜리 mini BERT 모델 제작과 학습이 정상적으로 진행됨\n",
    "    - 학습된 모델 및 학습과정의 시각화 내역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5574a1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
