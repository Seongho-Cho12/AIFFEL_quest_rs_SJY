{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02b136b4",
   "metadata": {},
   "source": [
    "# 가설\n",
    "“단일 모델보다 ensemble 모델이 더 높은 정확도와 f1-score를 보이며, num_words를 적절히 설정했을 때 모델 성능이 최적화된다.” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf4cfbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn \n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c9e683",
   "metadata": {},
   "source": [
    "# 빈도수 상위n개 단어 설정\n",
    "- 5000\n",
    "- 10000\n",
    "- 15000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e8d7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_words = 5000\n",
    "from keras.datasets import reuters\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=5000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871f0bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0]) # 인코딩된 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92808d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 수 : 8982\n",
      "테스트 샘플의 수 : 2246\n"
     ]
    }
   ],
   "source": [
    "print('훈련 샘플의 수 : {}'.format(len(x_train)))\n",
    "print('테스트 샘플의 수 : {}'.format(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7fc769d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스의 수 : 46\n"
     ]
    }
   ],
   "source": [
    "#클래스 개수\n",
    "num_classes = max(y_train) + 1\n",
    "print('클래스의 수 : {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e62930",
   "metadata": {},
   "source": [
    "## 데이터 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7070dd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 뉴스의 최대 길이 : 2376\n",
      "훈련용 뉴스의 평균 길이 : 145.5398574927633\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 클래스 빈도수:\n",
      "[[   0    1    2    3    4    5    6    7    8    9   10   11   12   13\n",
      "    14   15   16   17   18   19   20   21   22   23   24   25   26   27\n",
      "    28   29   30   31   32   33   34   35   36   37   38   39   40   41\n",
      "    42   43   44   45]\n",
      " [  55  432   74 3159 1949   17   48   16  139  101  124  390   49  172\n",
      "    26   20  444   39   66  549  269  100   15   41   62   92   24   15\n",
      "    48   19   45   39   32   11   50   10   49   19   19   24   36   30\n",
      "    13   21   12   18]]\n"
     ]
    }
   ],
   "source": [
    "print('훈련용 뉴스의 최대 길이 : {}'.format(max(len(l) for l in x_train)))\n",
    "print('훈련용 뉴스의 평균 길이 : {}'.format(sum(map(len, x_train))/len(x_train)))\n",
    "plt.hist([len(s) for s in x_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "fig, axe = plt.subplots(ncols=1)\n",
    "fig.set_size_inches(11,5)\n",
    "sns.countplot(x=y_train)\n",
    "plt.show()\n",
    "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
    "print(\"각 클래스 빈도수:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7726e071",
   "metadata": {},
   "source": [
    "## 원본 뉴스로 복원하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65f81b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#정수 시퀸스로 되어있는 데이터를 텍스트 형태로 돌리기 위해 index_word 생성\n",
    "word_index = reuters.get_word_index(path = 'reuters_word_index.json')\n",
    "index_to_word = {index + 3 : word for word, index in word_index.items()}\n",
    "# print(index_to_word[4])\n",
    "# print(index_to_word[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8f593fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0은 <pad>, 1은<sos>, 2는 <unk>\n",
    "for index, token in enumerate(('<pad>', '<sos>', '<unk>')):\n",
    "    index_to_word[index] = token\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0963bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(' '.join([index_to_word[index] for index in x_train[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b0653c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246\n",
      "8982\n"
     ]
    }
   ],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "    decoded.append(t)\n",
    "x_train = decoded\n",
    "decoded = []\n",
    "for i in range(len(x_test)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test = decoded\n",
    "print(len(x_test))\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8e1361a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> <unk> <unk> said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3',\n",
       " '<sos> generale de banque sa lt <unk> <unk> and lt heller overseas corp of chicago have each taken 50 pct stakes in <unk> company sa <unk> factors generale de banque said in a statement it gave no financial details of the transaction sa <unk> <unk> turnover in 1986 was 17 5 billion belgian francs reuter 3']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f648472d",
   "metadata": {},
   "source": [
    "## 벡터화 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27b9dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8b2ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_tmp = [item[1] for item in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa622fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 4867)\n",
      "(8982, 4867)\n"
     ]
    }
   ],
   "source": [
    "# DTM 생성\n",
    "dtmvector = CountVectorizer()\n",
    "x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "print(x_train_dtm.shape)\n",
    "\n",
    "#TF-IDF생성\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "print(tfidfv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc649b",
   "metadata": {},
   "source": [
    "# 1. 나이브 베이즈 분류기\n",
    "- num_words = 5000\n",
    "- 정확도: 0.6731967943009796\n",
    "- f1-score:  0.6012501291711391"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2de9622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2284f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tfidfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8651c1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB = MultinomialNB()\n",
    "NB.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3660bf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6731967943009796\n",
      "f1-score:  0.6012501291711391\n"
     ]
    }
   ],
   "source": [
    "x_test_dtm = dtmvector.transform(x_test) #테스트 데이터를 DTM으로 변환\n",
    "tfidfv_test = tfidf_transformer.transform(x_test_dtm) #DTM을 TF-IDF 행렬로 변환\n",
    "\n",
    "nb_predicted = NB.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, nb_predicted)) #예측값과 실제값 비교\n",
    "print('f1-score: ', f1_score(y_test, nb_predicted, average='weighted'))  # f1-score 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3eecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_3 = NB.predict_proba(tfidfv_test[3])[0]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (11,5)\n",
    "plt.bar(NB.classes_, probability_3)\n",
    "plt.xlim(-1, 21)\n",
    "plt.xticks(NB.classes_)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2310cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB.predict(tfidfv_test[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed833e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = f1_score(y_test, y_pred, average='weighted')  # f1-score 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9010d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# def graph_confusion_matrix(model, x_test, y_test):#, classes_name):\n",
    "#     y_pred = model.predict(x_test)\n",
    "#     cm = confusion_matrix(y_test, y_pred)\n",
    "#     df_cm = pd.DataFrame(cm)\n",
    "    \n",
    "#     plt.figure(figsize=(12, 12))\n",
    "#     heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "#     heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=12)\n",
    "#     heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=12)\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "#     plt.title('Confusion Matrix')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a73ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_confusion_matrix(NB, tfidfv_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc67e7",
   "metadata": {},
   "source": [
    "# 1.1 나이브 베이즈 분류기\n",
    "- num_words=10000, accuracy=0.6589, f1-score=0.5782\n",
    "- num_words=15000, accuracy=0.6371, f1-score=0.5536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1674c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words=10000, accuracy=0.6589\n",
      "num_words=10000, f1-score=0.5782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.62      0.71      0.66       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.81      0.90      0.85       813\n",
      "           4       0.52      0.96      0.67       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.08      0.15        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.62      0.64      0.63        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       1.00      0.03      0.05        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.69      0.57      0.62        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.59      0.78      0.67       133\n",
      "          20       1.00      0.04      0.08        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.03      0.06        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.66      2246\n",
      "   macro avg       0.17      0.10      0.10      2246\n",
      "weighted avg       0.59      0.66      0.58      2246\n",
      "\n",
      "num_words=15000, accuracy=0.6371\n",
      "num_words=15000, f1-score=0.5536\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.75      0.54      0.63       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.78      0.91      0.84       813\n",
      "           4       0.48      0.96      0.64       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.04      0.08        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.70      0.42      0.53        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.75      0.43      0.55        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.64      0.75      0.69       133\n",
      "          20       1.00      0.01      0.03        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.00      0.00      0.00        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.64      2246\n",
      "   macro avg       0.13      0.09      0.09      2246\n",
      "weighted avg       0.56      0.64      0.55      2246\n",
      "\n",
      "{10000: 0.6589492430988424, 15000: 0.6371326803205699}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "\n",
    "\n",
    "def decode_review(sequence, reverse_word_index):\n",
    "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in sequence])\n",
    "\n",
    "results = {}\n",
    "\n",
    "for num_words in [10000, 15000]:\n",
    "    # 데이터 로드\n",
    "    (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_words, test_split=0.2)\n",
    "    word_index = reuters.get_word_index()\n",
    "    reverse_word_index = {v: k for k, v in word_index.items()}\n",
    "    for index, token in enumerate(('<pad>', '<sos>', '<unk>')):\n",
    "        reverse_word_index[index] = token\n",
    "\n",
    "    # 숫자 → 텍스트 변환\n",
    "    x_train_text = [decode_review(seq, reverse_word_index) for seq in x_train]\n",
    "    x_test_text = [decode_review(seq, reverse_word_index) for seq in x_test]\n",
    "\n",
    "    # DTM + TF-IDF\n",
    "    vectorizer = CountVectorizer()\n",
    "    x_train_dtm = vectorizer.fit_transform(x_train_text)\n",
    "    x_test_dtm = vectorizer.transform(x_test_text)\n",
    "\n",
    "    tfidf = TfidfTransformer()\n",
    "    x_train_tfidf = tfidf.fit_transform(x_train_dtm)\n",
    "    x_test_tfidf = tfidf.transform(x_test_dtm)\n",
    "\n",
    "    # 모델 학습\n",
    "    model = MultinomialNB()\n",
    "    model.fit(x_train_tfidf, y_train)\n",
    "\n",
    "    # 예측 및 평가\n",
    "    y_pred = model.predict(x_test_tfidf)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')  # f1-score 추가\n",
    "    results[num_words] = acc\n",
    "    print(f'num_words={num_words}, accuracy={acc:.4f}')\n",
    "    print(f'num_words={num_words}, f1-score={f1:.4f}')\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "# 결과 확인\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a414a7a7",
   "metadata": {},
   "source": [
    "# 2. Voting\n",
    "- soft/hard voting  \n",
    "\n",
    "    확률 기반 : naive bayes, complement naive bayes  \n",
    "    \n",
    "    선형 기반 : logistic regression, svm  \n",
    "    \n",
    "    트리 기반 : decision tree, random forest, gradient boosting tree  \n",
    "    \n",
    "    \n",
    "- Naive Bayes + Logistic Regression + Random Forest\n",
    "- Naive Bayes + svm + Random Forest\n",
    "- Complement Naive Bayes + SVM + Random Forest\n",
    "- Logistic Regression + SVM + Gradient Boosting Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00d4c2",
   "metadata": {},
   "source": [
    "## 2.1 Naive Bayes + Logistic Regression + Random Forest\n",
    "- num_words=5000, f1-score=0.7222, accuracy=0.7560\n",
    "- num_words=10000, f1-score=0.6962, accuracy=0.7369\n",
    "- num_words=15000, f1-score=0.6781, accuracy=0.7222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbcdff85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words=5000, f1-score=0.7222\n",
      "num_words=5000, accuracy=0.7560\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.50      0.63        12\n",
      "           1       0.58      0.83      0.68       105\n",
      "           2       1.00      0.25      0.40        20\n",
      "           3       0.91      0.92      0.91       813\n",
      "           4       0.67      0.94      0.78       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.50      0.67        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.81      0.45      0.58        38\n",
      "           9       0.86      0.76      0.81        25\n",
      "          10       0.94      0.57      0.71        30\n",
      "          11       0.57      0.83      0.68        83\n",
      "          12       0.50      0.08      0.13        13\n",
      "          13       0.70      0.43      0.53        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.65      0.76      0.70        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.79      0.55      0.65        20\n",
      "          19       0.60      0.82      0.69       133\n",
      "          20       0.88      0.31      0.46        70\n",
      "          21       0.84      0.59      0.70        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       1.00      0.05      0.10        19\n",
      "          25       0.92      0.35      0.51        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       1.00      0.08      0.15        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.30      0.46        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       1.00      0.43      0.60         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.50      0.09      0.15        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.67      0.80         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.76      2246\n",
      "   macro avg       0.51      0.31      0.35      2246\n",
      "weighted avg       0.75      0.76      0.72      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words=10000, f1-score=0.6962\n",
      "num_words=10000, accuracy=0.7369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.66      0.80      0.72       105\n",
      "           2       1.00      0.20      0.33        20\n",
      "           3       0.88      0.92      0.90       813\n",
      "           4       0.61      0.94      0.74       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.14      0.25        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.75      0.39      0.52        38\n",
      "           9       0.83      0.76      0.79        25\n",
      "          10       0.94      0.50      0.65        30\n",
      "          11       0.67      0.75      0.70        83\n",
      "          12       0.67      0.15      0.25        13\n",
      "          13       0.60      0.24      0.35        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.68      0.72      0.70        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.89      0.40      0.55        20\n",
      "          19       0.63      0.80      0.71       133\n",
      "          20       0.86      0.27      0.41        70\n",
      "          21       0.82      0.52      0.64        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       1.00      0.05      0.10        19\n",
      "          25       1.00      0.26      0.41        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.10      0.18        10\n",
      "          33       1.00      0.40      0.57         5\n",
      "          34       1.00      0.29      0.44         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.50      0.67         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.74      2246\n",
      "   macro avg       0.49      0.26      0.30      2246\n",
      "weighted avg       0.72      0.74      0.70      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words=15000, f1-score=0.6781\n",
      "num_words=15000, accuracy=0.7222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.50      0.63        12\n",
      "           1       0.70      0.77      0.73       105\n",
      "           2       1.00      0.15      0.26        20\n",
      "           3       0.85      0.92      0.89       813\n",
      "           4       0.57      0.95      0.71       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.14      0.25        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.71      0.39      0.51        38\n",
      "           9       0.94      0.60      0.73        25\n",
      "          10       0.93      0.47      0.62        30\n",
      "          11       0.74      0.71      0.72        83\n",
      "          12       1.00      0.15      0.27        13\n",
      "          13       0.50      0.16      0.24        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.70      0.64      0.67        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       1.00      0.35      0.52        20\n",
      "          19       0.68      0.76      0.72       133\n",
      "          20       0.95      0.26      0.40        70\n",
      "          21       0.81      0.48      0.60        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.50      0.05      0.10        19\n",
      "          25       1.00      0.19      0.32        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.10      0.18        10\n",
      "          33       1.00      0.20      0.33         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.33      0.50         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.72      2246\n",
      "   macro avg       0.47      0.23      0.27      2246\n",
      "weighted avg       0.71      0.72      0.68      2246\n",
      "\n",
      "{10000: 0.7368655387355298, 15000: 0.7221727515583259, 5000: 0.7560106856634016}\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes + Logistic Regression + Random Forest\n",
    "for num_words in [5000, 10000, 15000]:\n",
    "    # 데이터 로드\n",
    "    (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_words, test_split=0.2)\n",
    "    word_index = reuters.get_word_index()\n",
    "    reverse_word_index = {v: k for k, v in word_index.items()}\n",
    "    for index, token in enumerate(('<pad>', '<sos>', '<unk>')):\n",
    "        reverse_word_index[index] = token\n",
    "    # 숫자 → 텍스트 변환\n",
    "    x_train_text = [decode_review(seq, reverse_word_index) for seq in x_train]\n",
    "    x_test_text = [decode_review(seq, reverse_word_index) for seq in x_test]\n",
    "\n",
    "    # DTM + TF-IDF\n",
    "    vectorizer = CountVectorizer()\n",
    "    x_train_dtm = vectorizer.fit_transform(x_train_text)\n",
    "    x_test_dtm = vectorizer.transform(x_test_text)\n",
    "\n",
    "    tfidf = TfidfTransformer()\n",
    "    x_train_tfidf = tfidf.fit_transform(x_train_dtm)\n",
    "    x_test_tfidf = tfidf.transform(x_test_dtm)\n",
    "\n",
    "    # 모델 학습\n",
    "    model = voting_clf = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('nb', MultinomialNB()),\n",
    "            ('lr', LogisticRegression()),\n",
    "            ('rf', RandomForestClassifier())\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "    model.fit(x_train_tfidf, y_train)\n",
    "\n",
    "    # 예측 및 평가\n",
    "    y_pred = model.predict(x_test_tfidf)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[num_words] = acc\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')  # f1-score 추가\n",
    "    print(f'num_words={num_words}, f1-score={f1:.4f}')\n",
    "    print(f'num_words={num_words}, accuracy={acc:.4f}')\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "# 결과 확인\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b78190c",
   "metadata": {},
   "source": [
    "## 2.2 Naive Bayes + svm + Random Forest\n",
    "- svm 모델이 predict_proba를 지원하지 않으므로 voting = 'hard'\n",
    "- num_words=5000, f1-score=0.7510, accuracy=0.7752\n",
    "- num_words=10000, f1-score=0.7310, accuracy=0.7582\n",
    "- num_words=15000, f1-score=0.7229, accuracy=0.7507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b31ac1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words=5000, f1-score=0.7510\n",
      "num_words=5000, accuracy=0.7752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.50      0.63        12\n",
      "           1       0.56      0.84      0.67       105\n",
      "           2       0.85      0.55      0.67        20\n",
      "           3       0.91      0.92      0.92       813\n",
      "           4       0.70      0.92      0.79       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.64      0.78        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.75      0.71      0.73        38\n",
      "           9       0.85      0.68      0.76        25\n",
      "          10       0.91      0.67      0.77        30\n",
      "          11       0.61      0.81      0.69        83\n",
      "          12       0.83      0.38      0.53        13\n",
      "          13       0.61      0.46      0.52        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.71      0.76      0.74        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.86      0.60      0.71        20\n",
      "          19       0.69      0.79      0.74       133\n",
      "          20       0.84      0.37      0.51        70\n",
      "          21       0.79      0.70      0.75        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.50      0.11      0.17        19\n",
      "          25       1.00      0.55      0.71        31\n",
      "          26       1.00      0.25      0.40         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.50      0.25      0.33         4\n",
      "          30       1.00      0.25      0.40        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.30      0.46        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.29      0.44         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.50      0.09      0.15        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.80      0.67      0.73         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.78      2246\n",
      "   macro avg       0.63      0.40      0.45      2246\n",
      "weighted avg       0.77      0.78      0.75      2246\n",
      "\n",
      "num_words=10000, f1-score=0.7310\n",
      "num_words=10000, accuracy=0.7582\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.56      0.80      0.66       105\n",
      "           2       0.87      0.65      0.74        20\n",
      "           3       0.89      0.92      0.91       813\n",
      "           4       0.65      0.92      0.76       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.50      0.67        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.78      0.66      0.71        38\n",
      "           9       0.84      0.64      0.73        25\n",
      "          10       0.90      0.60      0.72        30\n",
      "          11       0.64      0.76      0.69        83\n",
      "          12       0.67      0.15      0.25        13\n",
      "          13       0.64      0.38      0.47        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.74      0.68      0.71        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.92      0.55      0.69        20\n",
      "          19       0.69      0.77      0.73       133\n",
      "          20       0.76      0.31      0.44        70\n",
      "          21       0.82      0.67      0.73        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.25      0.08      0.12        12\n",
      "          24       0.60      0.16      0.25        19\n",
      "          25       1.00      0.42      0.59        31\n",
      "          26       1.00      0.25      0.40         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       1.00      0.10      0.18        10\n",
      "          29       0.50      0.25      0.33         4\n",
      "          30       1.00      0.08      0.15        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.30      0.46        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.14      0.25         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.50      0.09      0.15        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.75      0.50      0.60         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.76      2246\n",
      "   macro avg       0.65      0.37      0.43      2246\n",
      "weighted avg       0.75      0.76      0.73      2246\n",
      "\n",
      "num_words=15000, f1-score=0.7229\n",
      "num_words=15000, accuracy=0.7507\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.50      0.60        12\n",
      "           1       0.56      0.78      0.65       105\n",
      "           2       0.92      0.55      0.69        20\n",
      "           3       0.88      0.92      0.90       813\n",
      "           4       0.64      0.92      0.75       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.57      0.73        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.76      0.66      0.70        38\n",
      "           9       0.84      0.64      0.73        25\n",
      "          10       0.90      0.60      0.72        30\n",
      "          11       0.67      0.70      0.68        83\n",
      "          12       0.67      0.15      0.25        13\n",
      "          13       0.55      0.32      0.41        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.75      0.68      0.71        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.92      0.55      0.69        20\n",
      "          19       0.70      0.75      0.73       133\n",
      "          20       0.79      0.31      0.45        70\n",
      "          21       0.77      0.63      0.69        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.25      0.08      0.12        12\n",
      "          24       0.50      0.11      0.17        19\n",
      "          25       1.00      0.39      0.56        31\n",
      "          26       1.00      0.12      0.22         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.50      0.25      0.33         4\n",
      "          30       1.00      0.17      0.29        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.20      0.33        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.29      0.44         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.50      0.09      0.15        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.50      0.12      0.20         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       0.75      0.50      0.60         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.75      2246\n",
      "   macro avg       0.64      0.36      0.42      2246\n",
      "weighted avg       0.75      0.75      0.72      2246\n",
      "\n",
      "{10000: 0.7582368655387355, 15000: 0.7506678539626002, 5000: 0.7751558325912734}\n"
     ]
    }
   ],
   "source": [
    "for num_words in [5000, 10000, 15000]:\n",
    "    # 데이터 로드\n",
    "    (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_words, test_split=0.2)\n",
    "    word_index = reuters.get_word_index()\n",
    "    reverse_word_index = {v: k for k, v in word_index.items()}\n",
    "    for index, token in enumerate(('<pad>', '<sos>', '<unk>')):\n",
    "        reverse_word_index[index] = token\n",
    "    # 숫자 → 텍스트 변환\n",
    "    x_train_text = [decode_review(seq, reverse_word_index) for seq in x_train]\n",
    "    x_test_text = [decode_review(seq, reverse_word_index) for seq in x_test]\n",
    "\n",
    "    # DTM + TF-IDF\n",
    "    vectorizer = CountVectorizer()\n",
    "    x_train_dtm = vectorizer.fit_transform(x_train_text)\n",
    "    x_test_dtm = vectorizer.transform(x_test_text)\n",
    "\n",
    "    tfidf = TfidfTransformer()\n",
    "    x_train_tfidf = tfidf.fit_transform(x_train_dtm)\n",
    "    x_test_tfidf = tfidf.transform(x_test_dtm)\n",
    "\n",
    "    # 모델 학습\n",
    "    model = voting_clf = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('nb', MultinomialNB()),\n",
    "            ('svm', LinearSVC()),\n",
    "            ('rf', RandomForestClassifier())\n",
    "        ],\n",
    "        voting='hard'\n",
    "    )\n",
    "    model.fit(x_train_tfidf, y_train)\n",
    "\n",
    "    # 예측 및 평가\n",
    "    y_pred = model.predict(x_test_tfidf)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')  # f1-score 추가\n",
    "    print(f'num_words={num_words}, f1-score={f1:.4f}')\n",
    "    results[num_words] = acc\n",
    "    print(f'num_words={num_words}, accuracy={acc:.4f}')\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "# 결과 확인\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f4b201",
   "metadata": {},
   "source": [
    "## 2.3 Complement Naive Bayes + SVM + Random Forest\n",
    "- num_words=5000, f1-score=0.7794, accuracy=0.7979\n",
    "- num_words=10000, f1-score=0.7828, accuracy=0.8010\n",
    "- num_words=15000, f1-score=0.7809, accuracy=0.7983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18797874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words=5000, f1-score=0.7794\n",
      "num_words=5000, accuracy=0.7979\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.58      0.67        12\n",
      "           1       0.59      0.85      0.70       105\n",
      "           2       0.81      0.65      0.72        20\n",
      "           3       0.92      0.92      0.92       813\n",
      "           4       0.76      0.92      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.87      0.93      0.90        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.73      0.71      0.72        38\n",
      "           9       0.85      0.88      0.86        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.62      0.82      0.71        83\n",
      "          12       0.67      0.15      0.25        13\n",
      "          13       0.69      0.59      0.64        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.73      0.77      0.75        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.71      0.60      0.65        20\n",
      "          19       0.67      0.77      0.72       133\n",
      "          20       0.79      0.39      0.52        70\n",
      "          21       0.80      0.74      0.77        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.67      0.50      0.57        12\n",
      "          24       0.50      0.11      0.17        19\n",
      "          25       0.88      0.68      0.76        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.20      0.29        10\n",
      "          29       0.50      0.25      0.33         4\n",
      "          30       1.00      0.42      0.59        12\n",
      "          31       1.00      0.15      0.27        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.57      0.73         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.50      0.09      0.15        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       0.80      0.67      0.73         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.80      2246\n",
      "   macro avg       0.71      0.49      0.54      2246\n",
      "weighted avg       0.79      0.80      0.78      2246\n",
      "\n",
      "num_words=10000, f1-score=0.7828\n",
      "num_words=10000, accuracy=0.8010\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73        12\n",
      "           1       0.61      0.87      0.72       105\n",
      "           2       0.80      0.60      0.69        20\n",
      "           3       0.93      0.93      0.93       813\n",
      "           4       0.76      0.91      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.74      0.66      0.69        38\n",
      "           9       0.84      0.84      0.84        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.64      0.83      0.73        83\n",
      "          12       0.67      0.15      0.25        13\n",
      "          13       0.69      0.59      0.64        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.74      0.76      0.75        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.75      0.60      0.67        20\n",
      "          19       0.66      0.77      0.72       133\n",
      "          20       0.72      0.37      0.49        70\n",
      "          21       0.78      0.78      0.78        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.67      0.50      0.57        12\n",
      "          24       0.40      0.11      0.17        19\n",
      "          25       0.89      0.77      0.83        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.30      0.37        10\n",
      "          29       0.50      0.25      0.33         4\n",
      "          30       1.00      0.08      0.15        12\n",
      "          31       1.00      0.23      0.38        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.57      0.73         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.50      0.09      0.15        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       0.83      0.83      0.83         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.80      2246\n",
      "   macro avg       0.73      0.50      0.55      2246\n",
      "weighted avg       0.80      0.80      0.78      2246\n",
      "\n",
      "num_words=15000, f1-score=0.7809\n",
      "num_words=15000, accuracy=0.7983\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73        12\n",
      "           1       0.61      0.87      0.72       105\n",
      "           2       0.80      0.60      0.69        20\n",
      "           3       0.93      0.93      0.93       813\n",
      "           4       0.76      0.91      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.71      0.66      0.68        38\n",
      "           9       0.84      0.84      0.84        25\n",
      "          10       0.96      0.87      0.91        30\n",
      "          11       0.59      0.78      0.67        83\n",
      "          12       0.75      0.23      0.35        13\n",
      "          13       0.68      0.62      0.65        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.78      0.77      0.77        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.71      0.60      0.65        20\n",
      "          19       0.65      0.77      0.71       133\n",
      "          20       0.76      0.36      0.49        70\n",
      "          21       0.76      0.70      0.73        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.50      0.25      0.33        12\n",
      "          24       0.40      0.11      0.17        19\n",
      "          25       0.92      0.77      0.84        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.40      0.20      0.27        10\n",
      "          29       0.50      0.25      0.33         4\n",
      "          30       1.00      0.25      0.40        12\n",
      "          31       1.00      0.23      0.38        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.57      0.73         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.50      0.09      0.15        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       0.83      0.83      0.83         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.80      2246\n",
      "   macro avg       0.73      0.49      0.54      2246\n",
      "weighted avg       0.80      0.80      0.78      2246\n",
      "\n",
      "{10000: 0.800979519145147, 15000: 0.7983081032947462, 5000: 0.7978628673196795}\n"
     ]
    }
   ],
   "source": [
    "for num_words in [5000, 10000, 15000]:\n",
    "    # 데이터 로드\n",
    "    (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_words, test_split=0.2)\n",
    "    word_index = reuters.get_word_index()\n",
    "    reverse_word_index = {v: k for k, v in word_index.items()}\n",
    "    for index, token in enumerate(('<pad>', '<sos>', '<unk>')):\n",
    "        reverse_word_index[index] = token\n",
    "    # 숫자 → 텍스트 변환\n",
    "    x_train_text = [decode_review(seq, reverse_word_index) for seq in x_train]\n",
    "    x_test_text = [decode_review(seq, reverse_word_index) for seq in x_test]\n",
    "\n",
    "    # DTM + TF-IDF\n",
    "    vectorizer = CountVectorizer()\n",
    "    x_train_dtm = vectorizer.fit_transform(x_train_text)\n",
    "    x_test_dtm = vectorizer.transform(x_test_text)\n",
    "\n",
    "    tfidf = TfidfTransformer()\n",
    "    x_train_tfidf = tfidf.fit_transform(x_train_dtm)\n",
    "    x_test_tfidf = tfidf.transform(x_test_dtm)\n",
    "\n",
    "    # 모델 학습\n",
    "    model = voting_clf = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('cb', ComplementNB()),\n",
    "            ('svm', LinearSVC()),\n",
    "            ('rf', RandomForestClassifier())\n",
    "        ],\n",
    "        voting='hard'\n",
    "    )\n",
    "    model.fit(x_train_tfidf, y_train)\n",
    "\n",
    "    # 예측 및 평가\n",
    "    y_pred = model.predict(x_test_tfidf)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[num_words] = acc\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')  # f1-score 추가\n",
    "    print(f'num_words={num_words}, f1-score={f1:.4f}')\n",
    "    print(f'num_words={num_words}, accuracy={acc:.4f}')\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "# 결과 확인\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c6778c",
   "metadata": {},
   "source": [
    "## 2.4 Logistic Regression + SVM + Gradient Boosting Tree\n",
    "- num_words=5000, f1-score=0.8100, accuracy=0.8215\n",
    "- num_words=10000, f1-score=0.8075 accuracy=0.8179\n",
    "- num_words=15000, f1-score=0.8088, accuracy=0.8197"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0df4e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words=5000, f1-score=0.8100\n",
      "num_words=5000, accuracy=0.8215\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.69      0.85      0.76       105\n",
      "           2       0.79      0.75      0.77        20\n",
      "           3       0.91      0.94      0.93       813\n",
      "           4       0.79      0.91      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       0.67      0.67      0.67         3\n",
      "           8       0.77      0.71      0.74        38\n",
      "           9       0.85      0.88      0.86        25\n",
      "          10       0.93      0.83      0.88        30\n",
      "          11       0.67      0.78      0.72        83\n",
      "          12       0.88      0.54      0.67        13\n",
      "          13       0.65      0.59      0.62        37\n",
      "          14       1.00      1.00      1.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.75      0.78      0.76        99\n",
      "          17       0.83      0.42      0.56        12\n",
      "          18       0.78      0.70      0.74        20\n",
      "          19       0.74      0.76      0.75       133\n",
      "          20       0.76      0.49      0.59        70\n",
      "          21       0.73      0.81      0.77        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.67      0.67      0.67        12\n",
      "          24       0.77      0.53      0.62        19\n",
      "          25       1.00      0.77      0.87        31\n",
      "          26       0.86      0.75      0.80         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.30      0.37        10\n",
      "          29       0.67      0.50      0.57         4\n",
      "          30       1.00      0.50      0.67        12\n",
      "          31       0.75      0.23      0.35        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.57      0.73         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.75      0.27      0.40        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       0.75      0.50      0.60         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.82      2246\n",
      "   macro avg       0.76      0.57      0.62      2246\n",
      "weighted avg       0.82      0.82      0.81      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words=10000, f1-score=0.8075\n",
      "num_words=10000, accuracy=0.8179\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73        12\n",
      "           1       0.71      0.83      0.76       105\n",
      "           2       0.80      0.80      0.80        20\n",
      "           3       0.91      0.94      0.93       813\n",
      "           4       0.77      0.91      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.86      0.86        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.75      0.71      0.73        38\n",
      "           9       0.91      0.84      0.87        25\n",
      "          10       0.93      0.83      0.88        30\n",
      "          11       0.67      0.78      0.72        83\n",
      "          12       0.88      0.54      0.67        13\n",
      "          13       0.71      0.59      0.65        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.78      0.78      0.78        99\n",
      "          17       0.60      0.25      0.35        12\n",
      "          18       0.82      0.70      0.76        20\n",
      "          19       0.71      0.74      0.72       133\n",
      "          20       0.77      0.47      0.58        70\n",
      "          21       0.72      0.78      0.75        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.62      0.67      0.64        12\n",
      "          24       0.71      0.53      0.61        19\n",
      "          25       0.96      0.71      0.81        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.75      0.75      0.75         4\n",
      "          30       1.00      0.42      0.59        12\n",
      "          31       1.00      0.31      0.47        13\n",
      "          32       1.00      0.90      0.95        10\n",
      "          33       0.75      0.60      0.67         5\n",
      "          34       1.00      0.57      0.73         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.75      0.27      0.40        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.75      0.38      0.50         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.80      0.67      0.73         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.82      2246\n",
      "   macro avg       0.80      0.58      0.64      2246\n",
      "weighted avg       0.82      0.82      0.81      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words=15000, f1-score=0.8088\n",
      "num_words=15000, accuracy=0.8197\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.68      0.85      0.75       105\n",
      "           2       0.83      0.75      0.79        20\n",
      "           3       0.91      0.94      0.93       813\n",
      "           4       0.78      0.91      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.77      0.71      0.74        38\n",
      "           9       0.91      0.84      0.87        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.67      0.77      0.72        83\n",
      "          12       0.88      0.54      0.67        13\n",
      "          13       0.71      0.59      0.65        37\n",
      "          14       1.00      0.50      0.67         2\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.79      0.80      0.79        99\n",
      "          17       0.80      0.33      0.47        12\n",
      "          18       0.78      0.70      0.74        20\n",
      "          19       0.73      0.73      0.73       133\n",
      "          20       0.73      0.47      0.57        70\n",
      "          21       0.73      0.81      0.77        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.58      0.58      0.58        12\n",
      "          24       0.75      0.47      0.58        19\n",
      "          25       0.89      0.77      0.83        31\n",
      "          26       0.86      0.75      0.80         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.75      0.75      0.75         4\n",
      "          30       1.00      0.42      0.59        12\n",
      "          31       1.00      0.38      0.56        13\n",
      "          32       1.00      0.90      0.95        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.57      0.73         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.75      0.27      0.40        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.75      0.38      0.50         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.75      0.50      0.60         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.82      2246\n",
      "   macro avg       0.79      0.57      0.63      2246\n",
      "weighted avg       0.82      0.82      0.81      2246\n",
      "\n",
      "{10000: 0.8178984861976848, 15000: 0.819679430097952, 5000: 0.821460373998219}\n"
     ]
    }
   ],
   "source": [
    "for num_words in [5000, 10000, 15000]:\n",
    "    # 데이터 로드\n",
    "    (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_words, test_split=0.2)\n",
    "    word_index = reuters.get_word_index()\n",
    "    reverse_word_index = {v: k for k, v in word_index.items()}\n",
    "    for index, token in enumerate(('<pad>', '<sos>', '<unk>')):\n",
    "        reverse_word_index[index] = token\n",
    "    # 숫자 → 텍스트 변환\n",
    "    x_train_text = [decode_review(seq, reverse_word_index) for seq in x_train]\n",
    "    x_test_text = [decode_review(seq, reverse_word_index) for seq in x_test]\n",
    "\n",
    "    # DTM + TF-IDF\n",
    "    vectorizer = CountVectorizer()\n",
    "    x_train_dtm = vectorizer.fit_transform(x_train_text)\n",
    "    x_test_dtm = vectorizer.transform(x_test_text)\n",
    "\n",
    "    tfidf = TfidfTransformer()\n",
    "    x_train_tfidf = tfidf.fit_transform(x_train_dtm)\n",
    "    x_test_tfidf = tfidf.transform(x_test_dtm)\n",
    "\n",
    "    # 모델 학습\n",
    "    model = voting_clf = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression()),\n",
    "            ('svm', LinearSVC()),\n",
    "            ('grbt', GradientBoostingClassifier())\n",
    "        ],\n",
    "        voting='hard'\n",
    "    )\n",
    "    model.fit(x_train_tfidf, y_train)\n",
    "\n",
    "    # 예측 및 평가\n",
    "    y_pred = model.predict(x_test_tfidf)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[num_words] = acc\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')  # f1-score 추가\n",
    "    print(f'num_words={num_words}, f1-score={f1:.4f}')\n",
    "    print(f'num_words={num_words}, accuracy={acc:.4f}')\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "# 결과 확인\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89b0bb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, num_words_list = [5000, 10000, 15000]):\n",
    "    results = {}\n",
    "    f1_results = {}\n",
    "    \n",
    "    for num_words in num_words_list:\n",
    "        # 데이터 로드\n",
    "        (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_words, test_split=0.2)\n",
    "        word_index = reuters.get_word_index()\n",
    "        reverse_word_index = {v: k for k, v in word_index.items()}\n",
    "        for index, token in enumerate(('<pad>', '<sos>', '<unk>')):\n",
    "            reverse_word_index[index] = token\n",
    "        \n",
    "        # 숫자 → 텍스트 변환\n",
    "        x_train_text = [decode_review(seq, reverse_word_index) for seq in x_train]\n",
    "        x_test_text = [decode_review(seq, reverse_word_index) for seq in x_test]\n",
    "\n",
    "        # DTM + TF-IDF\n",
    "        vectorizer = CountVectorizer()\n",
    "        x_train_dtm = vectorizer.fit_transform(x_train_text)\n",
    "        x_test_dtm = vectorizer.transform(x_test_text)\n",
    "\n",
    "        tfidf = TfidfTransformer()\n",
    "        x_train_tfidf = tfidf.fit_transform(x_train_dtm)\n",
    "        x_test_tfidf = tfidf.transform(x_test_dtm)\n",
    "\n",
    "        # 모델 학습\n",
    "        model.fit(x_train_tfidf, y_train)\n",
    "\n",
    "        # 예측 및 평가\n",
    "        y_pred = model.predict(x_test_tfidf)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "        results[num_words] = acc\n",
    "        f1_results[num_words] = f1\n",
    "    \n",
    "        print(f'num_words={num_words}, accuracy={acc:.4f}, f1-score={f1:.4f}')\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    return results, f1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f4e76",
   "metadata": {},
   "source": [
    "# 전체 결과\n",
    "| 모델           | num\\_words | Accuracy | F1-score |\n",
    "| -------------- | ---------- | -------- | -------- |\n",
    "| Naive Bayes    | 5000       | 0.6732   | 0.6013   |\n",
    "| Naive Bayes    | 10000      | 0.6589   | 0.5782   |\n",
    "| Naive Bayes    | 15000      | 0.6371   | 0.5536   |\n",
    "| NB + LR + RF   | 5000       | 0.7560   | 0.7222   |\n",
    "| NB + LR + RF   | 10000      | 0.7369   | 0.6962   |\n",
    "| NB + LR + RF   | 15000      | 0.7222   | 0.6781   |\n",
    "| NB + SVM + RF  | 5000       | 0.7752   | 0.7510   |\n",
    "| NB + SVM + RF  | 10000      | 0.7582   | 0.7310   |\n",
    "| NB + SVM + RF  | 15000      | 0.7507   | 0.7229   |\n",
    "| CNB + SVM + RF | 5000       | 0.7979   | 0.7794   |\n",
    "| CNB + SVM + RF | 10000      | 0.8010   | 0.7828   |\n",
    "| CNB + SVM + RF | 15000      | 0.7983   | 0.7809   |\n",
    "| LR + SVM + GBT | 5000       | 0.8215   | 0.8100   |\n",
    "| LR + SVM + GBT | 10000      | 0.8179   | 0.8075   |\n",
    "| LR + SVM + GBT | 15000      | 0.8197   | 0.8088   |\n",
    "\n",
    "- Voting > 단일 모델: Ensemble의 강력함 입증\n",
    "- num_words는 5000~10000이 optimal, 15000은 성능 개선 거의 없음 + 비용 증가\n",
    "    - 단어 집합이 커질수록 희소성 증가, noise 유입\n",
    "    - voting 모델들은 하락폭이 작아 robust\n",
    "- Naive Bayes: f1-score가 accuracy보다 훨씬 낮음 -> 소수 클래스 예측 실패\n",
    "- voting 모델은 f1-score도 높음 -> 전체 클래스 예측 균형을 잘 맞춤\n",
    "- LR + SVM + GBT + 5000 num_words 최고 성능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952f83c5",
   "metadata": {},
   "source": [
    "###  모델별 성능 추이\n",
    "| 모델             | 5000 | 10000 | 15000  |\n",
    "| -------------- | ---- | ----- | ------ |\n",
    "| Naive Bayes    | 최고   | ↓ 감소  | ↓ 더 감소 |\n",
    "| NB + LR + RF   | 최고   | ↓ 감소  | ↓ 더 감소 |\n",
    "| NB + SVM + RF  | 최고   | ↓ 감소  | ↓ 더 감소 |\n",
    "| CNB + SVM + RF | 비슷   | 소폭 상승 | 유지     |\n",
    "| LR + SVM + GBT | 최고   | 미세 하락 | 유지     |\n",
    "\n",
    "\n",
    "- 공통적으로 5000에서 최고 성능\n",
    "- CNB + SVM + RF과 LR + SVM + GBT의 안정성 -> num_words 변화에 거의 영향을 받지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2effe5",
   "metadata": {},
   "source": [
    "# num_words = 3000로 줄여보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2ef075d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words=3000, accuracy=0.6910, f1-score=0.6334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.25      0.40        12\n",
      "           1       0.48      0.83      0.61       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.88      0.88      0.88       813\n",
      "           4       0.66      0.94      0.77       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.48      0.65        25\n",
      "          10       1.00      0.17      0.29        30\n",
      "          11       0.44      0.77      0.56        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.75      0.24      0.37        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.55      0.79      0.64        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.67      0.10      0.17        20\n",
      "          19       0.48      0.82      0.61       133\n",
      "          20       0.93      0.20      0.33        70\n",
      "          21       1.00      0.22      0.36        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.13      0.23        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.69      2246\n",
      "   macro avg       0.24      0.15      0.15      2246\n",
      "weighted avg       0.65      0.69      0.63      2246\n",
      "\n",
      "num_words=3000, accuracy=0.7676, f1-score=0.7380\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.57      0.85      0.68       105\n",
      "           2       1.00      0.35      0.52        20\n",
      "           3       0.93      0.92      0.92       813\n",
      "           4       0.71      0.93      0.81       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.50      0.67        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.78      0.55      0.65        38\n",
      "           9       0.83      0.76      0.79        25\n",
      "          10       0.95      0.63      0.76        30\n",
      "          11       0.55      0.84      0.67        83\n",
      "          12       0.50      0.08      0.13        13\n",
      "          13       0.68      0.51      0.58        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.61      0.77      0.68        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.71      0.60      0.65        20\n",
      "          19       0.60      0.83      0.70       133\n",
      "          20       0.88      0.33      0.48        70\n",
      "          21       0.80      0.59      0.68        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       1.00      0.05      0.10        19\n",
      "          25       0.93      0.45      0.61        31\n",
      "          26       1.00      0.12      0.22         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       1.00      0.17      0.29        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.60      0.75        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       1.00      0.57      0.73         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.50      0.09      0.15        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.50      0.67         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.53      0.34      0.38      2246\n",
      "weighted avg       0.76      0.77      0.74      2246\n",
      "\n",
      "num_words=3000, accuracy=0.7845, f1-score=0.7624\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.55      0.85      0.67       105\n",
      "           2       0.87      0.65      0.74        20\n",
      "           3       0.92      0.92      0.92       813\n",
      "           4       0.73      0.92      0.81       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.71      0.83        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.71      0.63      0.67        38\n",
      "           9       0.86      0.72      0.78        25\n",
      "          10       0.92      0.77      0.84        30\n",
      "          11       0.60      0.83      0.70        83\n",
      "          12       0.75      0.23      0.35        13\n",
      "          13       0.61      0.51      0.56        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.70      0.79      0.74        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.85      0.55      0.67        20\n",
      "          19       0.66      0.79      0.72       133\n",
      "          20       0.83      0.36      0.50        70\n",
      "          21       0.84      0.78      0.81        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.40      0.17      0.24        12\n",
      "          24       0.50      0.11      0.17        19\n",
      "          25       1.00      0.48      0.65        31\n",
      "          26       1.00      0.38      0.55         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.67      0.50      0.57         4\n",
      "          30       1.00      0.17      0.29        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.50      0.67        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.43      0.60         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.50      0.09      0.15        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.50      0.12      0.20         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.67      0.80         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.78      2246\n",
      "   macro avg       0.68      0.43      0.49      2246\n",
      "weighted avg       0.78      0.78      0.76      2246\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words=3000, accuracy=0.8041, f1-score=0.7846\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.58      0.67        12\n",
      "           1       0.63      0.87      0.73       105\n",
      "           2       0.80      0.60      0.69        20\n",
      "           3       0.93      0.92      0.93       813\n",
      "           4       0.76      0.92      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.87      0.93      0.90        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.71      0.63      0.67        38\n",
      "           9       0.85      0.88      0.86        25\n",
      "          10       0.90      0.90      0.90        30\n",
      "          11       0.64      0.83      0.73        83\n",
      "          12       0.67      0.15      0.25        13\n",
      "          13       0.67      0.65      0.66        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.72      0.79      0.75        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.75      0.60      0.67        20\n",
      "          19       0.68      0.80      0.73       133\n",
      "          20       0.85      0.41      0.56        70\n",
      "          21       0.81      0.78      0.79        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.70      0.58      0.64        12\n",
      "          24       0.60      0.16      0.25        19\n",
      "          25       0.87      0.65      0.74        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.33      0.20      0.25        10\n",
      "          29       0.67      0.50      0.57         4\n",
      "          30       1.00      0.33      0.50        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.57      0.73         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.50      0.09      0.15        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.50      0.12      0.20         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.83      0.91         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.80      2246\n",
      "   macro avg       0.67      0.49      0.53      2246\n",
      "weighted avg       0.79      0.80      0.78      2246\n",
      "\n",
      "num_words=3000, accuracy=0.8192, f1-score=0.8089\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.68      0.84      0.75       105\n",
      "           2       0.75      0.75      0.75        20\n",
      "           3       0.91      0.94      0.93       813\n",
      "           4       0.79      0.91      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.78      0.66      0.71        38\n",
      "           9       0.85      0.88      0.86        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.68      0.77      0.72        83\n",
      "          12       0.86      0.46      0.60        13\n",
      "          13       0.65      0.59      0.62        37\n",
      "          14       1.00      0.50      0.67         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.74      0.79      0.76        99\n",
      "          17       0.86      0.50      0.63        12\n",
      "          18       0.76      0.65      0.70        20\n",
      "          19       0.72      0.75      0.74       133\n",
      "          20       0.74      0.50      0.60        70\n",
      "          21       0.69      0.81      0.75        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.73      0.67      0.70        12\n",
      "          24       0.73      0.42      0.53        19\n",
      "          25       0.92      0.77      0.84        31\n",
      "          26       0.86      0.75      0.80         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.30      0.37        10\n",
      "          29       0.50      0.25      0.33         4\n",
      "          30       1.00      0.50      0.67        12\n",
      "          31       1.00      0.23      0.38        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.57      0.73         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.67      0.36      0.47        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.75      0.38      0.50         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       1.00      0.67      0.80         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.82      2246\n",
      "   macro avg       0.81      0.57      0.63      2246\n",
      "weighted avg       0.82      0.82      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Naive Bayes\n",
    "results_nb, f1_nb = run_experiment(MultinomialNB(), num_words_list=[3000])\n",
    "\n",
    "# 2. NB + LR + RF\n",
    "model_nblrrf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('nb', MultinomialNB()),\n",
    "        ('lr', LogisticRegression(max_iter=1000)),\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "results_nblrrf, f1_nblrrf = run_experiment(model_nblrrf, num_words_list=[3000])\n",
    "\n",
    "# 3. NB + SVM + RF (SVM 때문에 hard voting)\n",
    "model_nbsvmrf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('nb', MultinomialNB()),\n",
    "        ('svm', LinearSVC()),\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "results_nbsvmrf, f1_nbsvmrf = run_experiment(model_nbsvmrf, num_words_list=[3000])\n",
    "\n",
    "# 4. CNB + SVM + RF\n",
    "model_cnbsvmrf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('cnb', ComplementNB()),\n",
    "        ('svm', LinearSVC()),\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "results_cnbsvmrf, f1_cnbsvmrf = run_experiment(model_cnbsvmrf, num_words_list=[3000])\n",
    "\n",
    "# 5. LR + SVM + GBT\n",
    "model_lrsvmgbt = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(max_iter=1000)),\n",
    "        ('svm', LinearSVC()),\n",
    "        ('gbt', GradientBoostingClassifier())\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "results_lrsvmgbt, f1_lrsvmgbt = run_experiment(model_lrsvmgbt, num_words_list=[3000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "144cfe9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Model  num_words  Accuracy  F1-score\n",
      "0  CNB + SVM + RF       3000    0.8041    0.7846\n",
      "1  LR + SVM + GBT       3000    0.8192    0.8089\n",
      "2    NB + LR + RF       3000    0.7676    0.7380\n",
      "3   NB + SVM + RF       3000    0.7845    0.7624\n",
      "4     Naive Bayes       3000    0.6910    0.6334\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 실험 결과를 하나로 통합\n",
    "all_results = {\n",
    "    'Naive Bayes': (results_nb, f1_nb),\n",
    "    'NB + LR + RF': (results_nblrrf, f1_nblrrf),\n",
    "    'NB + SVM + RF': (results_nbsvmrf, f1_nbsvmrf),\n",
    "    'CNB + SVM + RF': (results_cnbsvmrf, f1_cnbsvmrf),\n",
    "    'LR + SVM + GBT': (results_lrsvmgbt, f1_lrsvmgbt)\n",
    "}\n",
    "\n",
    "# ✅ DataFrame으로 변환\n",
    "rows = []\n",
    "for model_name, (acc_dict, f1_dict) in all_results.items():\n",
    "    for num_words in acc_dict.keys():\n",
    "        rows.append({\n",
    "            'Model': model_name,\n",
    "            'num_words': num_words,\n",
    "            'Accuracy': round(acc_dict[num_words], 4),\n",
    "            'F1-score': round(f1_dict[num_words], 4)\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(rows)\n",
    "results_df = results_df.sort_values(by=['Model', 'num_words']).reset_index(drop=True)\n",
    "\n",
    "# ✅ 결과 테이블 출력\n",
    "print(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf1ec7",
   "metadata": {},
   "source": [
    "# num_words 감소 효과 확인(3000 vs 5000~15000)\n",
    "| 모델             | 3000            | 5000~15000                     | 효과                |\n",
    "| -------------- | --------------- | ------------------------------- | ----------------- |\n",
    "| Naive Bayes    | 0.6910 / 0.6334 | 0.6371 ~ 0.6732 / 0.5536 ~ 0.6013 | 3000에서 **명확한 향상** |\n",
    "| NB + LR + RF   | 0.7676 / 0.7380 | 0.7222 ~ 0.7560 / 0.6781 ~ 0.7222 | **3000 최고**       |\n",
    "| NB + SVM + RF  | 0.7845 / 0.7624 | 0.7507 ~ 0.7752 / 0.7229 ~ 0.7510 | **3000 최고**       |\n",
    "| CNB + SVM + RF | 0.8041 / 0.7846 | 0.7979 ~ 0.8010 / 0.7794 ~ 0.7828 | 큰 차이 없음 (매우 안정)   |\n",
    "| LR + SVM + GBT | 0.8192 / 0.8089 | 0.8179 ~ 0.8215 / 0.8075 ~ 0.8100 | 거의 동일 (안정성 최고)    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b217d625",
   "metadata": {},
   "source": [
    "# 모델별 특성 요약\n",
    "| 모델             | 특성                           |\n",
    "| -------------- | ---------------------------- |\n",
    "| Naive Bayes    | 단일 모델 → feature 수 줄일 때 가장 개선 |\n",
    "| NB + LR + RF   | 기본 Voting → 3000에서 가장 강함     |\n",
    "| NB + SVM + RF  | SVM 추가 → 3000에서 가장 강함        |\n",
    "| CNB + SVM + RF | 불균형 데이터, 희소성 강인 → 매우 안정      |\n",
    "| LR + SVM + GBT | 최고 성능 + 최고 안정성 → 전체 Best 모델, 5000에서 가장 강함|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd003d4",
   "metadata": {},
   "source": [
    "# 딥러닝 모델 적용\n",
    "- maxlen = 300\n",
    "- model: lstm, num_words: 5000, accuracy: 0.6109, f1-score: 0.5635\n",
    "- model: bilstm, num_words: 5000, accuracy: 0.5850, f1-score: 0.5317\n",
    "- model: cnn, num_words: 5000, accuracy: 0.7912, f1-score: 0.7736\n",
    "- Logistic Regression + SVM + Gradient Boosting Tree\n",
    "    num_words=5000, f1-score=0.8100, accuracy=0.8215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7f5f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def run_deep_learning_experiment(num_words=5000, maxlen=300, model_type='lstm'):\n",
    "    # 1. 데이터 로드\n",
    "    (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_words, test_split=0.2)\n",
    "    \n",
    "    # 2. 시퀀스 패딩\n",
    "    x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "    x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "    # 3. 모델 생성\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=num_words, output_dim=128, input_length=maxlen))\n",
    "\n",
    "    if model_type == 'lstm':\n",
    "        model.add(LSTM(64))\n",
    "    elif model_type == 'bilstm':\n",
    "        from tensorflow.keras.layers import Bidirectional\n",
    "        model.add(Bidirectional(LSTM(64)))\n",
    "    elif model_type == 'cnn':\n",
    "        model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "    else:\n",
    "        raise ValueError(\"model_type은 'lstm', 'bilstm', 'cnn' 중 하나여야 합니다.\")\n",
    "    \n",
    "    model.add(Dense(46, activation='softmax'))  # Reuters는 46 classes\n",
    "\n",
    "    # 4. 컴파일 및 학습\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              epochs=10,\n",
    "              batch_size=128,\n",
    "              validation_split=0.2,\n",
    "              callbacks=[early_stop],\n",
    "              verbose=2)\n",
    "\n",
    "    # 5. 평가\n",
    "    y_pred_prob = model.predict(x_test)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    print(f\"model: {model_type}, num_words: {num_words}, accuracy: {acc:.4f}, f1-score: {f1:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    return acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "273f7deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 - 2s - loss: 2.8704 - accuracy: 0.3431 - val_loss: 2.4016 - val_accuracy: 0.3450\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 2.2590 - accuracy: 0.4203 - val_loss: 2.0723 - val_accuracy: 0.4942\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 2.0542 - accuracy: 0.4926 - val_loss: 1.9987 - val_accuracy: 0.5036\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 1.8724 - accuracy: 0.5296 - val_loss: 1.8380 - val_accuracy: 0.5203\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 1.6586 - accuracy: 0.5770 - val_loss: 1.6360 - val_accuracy: 0.5843\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 1.5361 - accuracy: 0.6134 - val_loss: 1.5533 - val_accuracy: 0.6144\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 1.4201 - accuracy: 0.6480 - val_loss: 1.6397 - val_accuracy: 0.5860\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 1.4663 - accuracy: 0.6262 - val_loss: 1.5403 - val_accuracy: 0.6283\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 1.2958 - accuracy: 0.6720 - val_loss: 1.4732 - val_accuracy: 0.6366\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 1.2504 - accuracy: 0.6839 - val_loss: 1.5487 - val_accuracy: 0.6166\n",
      "model: lstm, num_words: 5000, accuracy: 0.6109, f1-score: 0.5635\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.30      0.55      0.39       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.84      0.96      0.90       813\n",
      "           4       0.80      0.70      0.74       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.00      0.00      0.00        25\n",
      "          10       0.16      0.33      0.22        30\n",
      "          11       0.35      0.27      0.30        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.22      0.61      0.33        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.18      0.45      0.26        20\n",
      "          19       0.41      0.73      0.53       133\n",
      "          20       0.00      0.00      0.00        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.07      0.06      0.07        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.61      2246\n",
      "   macro avg       0.07      0.10      0.08      2246\n",
      "weighted avg       0.54      0.61      0.56      2246\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6108637577916296, 0.5634710670155165)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM 모델\n",
    "run_deep_learning_experiment(num_words=5000, model_type='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c47cedb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 - 4s - loss: 2.5719 - accuracy: 0.3777 - val_loss: 1.9945 - val_accuracy: 0.4814\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 1.8323 - accuracy: 0.5100 - val_loss: 1.9123 - val_accuracy: 0.4814\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 2.0774 - accuracy: 0.4618 - val_loss: 2.0946 - val_accuracy: 0.4886\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 2.0149 - accuracy: 0.5035 - val_loss: 1.9978 - val_accuracy: 0.5036\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 1.9113 - accuracy: 0.5165 - val_loss: 1.8514 - val_accuracy: 0.5186\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 1.7605 - accuracy: 0.5484 - val_loss: 1.7084 - val_accuracy: 0.5671\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 1.6564 - accuracy: 0.5846 - val_loss: 1.6658 - val_accuracy: 0.5732\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 1.6247 - accuracy: 0.5882 - val_loss: 1.6629 - val_accuracy: 0.5893\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 1.6102 - accuracy: 0.5986 - val_loss: 1.6769 - val_accuracy: 0.5815\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 1.5839 - accuracy: 0.6017 - val_loss: 1.6464 - val_accuracy: 0.5860\n",
      "model: bilstm, num_words: 5000, accuracy: 0.5850, f1-score: 0.5317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.08      0.09      0.08       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.85      0.94      0.89       813\n",
      "           4       0.75      0.87      0.80       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.00      0.00      0.00        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.19      0.16      0.17        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.25      0.24      0.25        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.22      0.16      0.18        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.18      0.67      0.28       133\n",
      "          20       0.00      0.00      0.00        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.00      0.00      0.00        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.59      2246\n",
      "   macro avg       0.05      0.07      0.06      2246\n",
      "weighted avg       0.50      0.59      0.53      2246\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.585040071237756, 0.5317431165409485)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bilstm 모델\n",
    "run_deep_learning_experiment(num_words=5000, model_type='bilstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "480e0bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 - 23s - loss: 2.5779 - accuracy: 0.4188 - val_loss: 1.8949 - val_accuracy: 0.5081\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 1.6875 - accuracy: 0.5896 - val_loss: 1.5068 - val_accuracy: 0.6706\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 1.3204 - accuracy: 0.6988 - val_loss: 1.2668 - val_accuracy: 0.7095\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 1.0792 - accuracy: 0.7457 - val_loss: 1.1424 - val_accuracy: 0.7385\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.9004 - accuracy: 0.7854 - val_loss: 1.0380 - val_accuracy: 0.7641\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.7490 - accuracy: 0.8195 - val_loss: 0.9635 - val_accuracy: 0.7774\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.6190 - accuracy: 0.8472 - val_loss: 0.9067 - val_accuracy: 0.7863\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.5065 - accuracy: 0.8745 - val_loss: 0.8733 - val_accuracy: 0.7986\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.4064 - accuracy: 0.8987 - val_loss: 0.8566 - val_accuracy: 0.8019\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 0.3264 - accuracy: 0.9200 - val_loss: 0.8517 - val_accuracy: 0.8036\n",
      "model: cnn, num_words: 5000, accuracy: 0.7912, f1-score: 0.7736\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.72      0.85      0.78       105\n",
      "           2       0.67      0.80      0.73        20\n",
      "           3       0.90      0.95      0.92       813\n",
      "           4       0.79      0.88      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.71      0.83        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.68      0.68      0.68        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.91      0.97      0.94        30\n",
      "          11       0.62      0.70      0.66        83\n",
      "          12       0.71      0.38      0.50        13\n",
      "          13       0.53      0.49      0.51        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.71      0.78      0.74        99\n",
      "          17       1.00      0.25      0.40        12\n",
      "          18       0.61      0.55      0.58        20\n",
      "          19       0.63      0.71      0.67       133\n",
      "          20       0.61      0.51      0.56        70\n",
      "          21       0.64      0.52      0.57        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.25      0.17      0.20        12\n",
      "          24       0.67      0.53      0.59        19\n",
      "          25       0.86      0.77      0.81        31\n",
      "          26       1.00      0.75      0.86         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.30      0.30      0.30        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.80      0.33      0.47        12\n",
      "          31       0.50      0.15      0.24        13\n",
      "          32       1.00      0.60      0.75        10\n",
      "          33       1.00      0.60      0.75         5\n",
      "          34       1.00      0.43      0.60         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.40      0.18      0.25        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.67      0.33      0.44         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.79      2246\n",
      "   macro avg       0.56      0.41      0.45      2246\n",
      "weighted avg       0.77      0.79      0.77      2246\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7911843276936776, 0.7735816624506641)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cnn 모델\n",
    "run_deep_learning_experiment(num_words=5000, model_type='cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e7f75f",
   "metadata": {},
   "source": [
    "# 딥러닝 모델과 비교\n",
    "| 모델                           | Accuracy   | F1-score   |\n",
    "| ---------------------------- | ---------- | ---------- |\n",
    "| LSTM                         | 0.6109     | 0.5635     |\n",
    "| Bi-LSTM                      | 0.5850     | 0.5317     |\n",
    "| CNN                          | 0.7912     | 0.7736     |\n",
    "| LR + SVM + GBT (ML voting) | **0.8215** | **0.8100** |\n",
    "\n",
    "- 딥러닝은 Reuters와 같은 small dataset에 적합하지 않음\n",
    "- RNN 계열 성능 저조\n",
    "- CNN의 상대적 선전\n",
    "- ML ensemble (LR + SVM + GBT) = 최종 Best Model  \n",
    "\n",
    "    highest accuracy\n",
    "\n",
    "    highest f1-score\n",
    "\n",
    "    highest stability across classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59440c43",
   "metadata": {},
   "source": [
    "# 회고\n",
    "- 뉴스 데이터셋에서 num_words와 voting 모델 조합에 따라 분류 성능이 어떻게 변화하는지 분석하였다. \n",
    "- num_words가 5000에서 최고 성능을 보일것이라고 예상했으나 3000으로 줄인 후에도 성능이 향상되거나 유지되었다. \n",
    "- voting에서 linearSVC때문에 soft voting을 일부 모델에서 사용하지 못했다. \n",
    "- soft/hard voting 비교 실험을 하면 좋을 것 같다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb50ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
