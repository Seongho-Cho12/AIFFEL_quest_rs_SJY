{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3efe6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "0.5.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import konlpy\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "# print(plt.__version__)\n",
    "print(konlpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec42a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SentencePiece, updated, version \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f894b5c",
   "metadata": {},
   "source": [
    "# SentencePiece 설치\n",
    "- Google에서 제공하는 오픈소스기반 Sentence Tokenizer/Detokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd26287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (0.1.96)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43977a1",
   "metadata": {},
   "source": [
    "# SentencePiece 모델 학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d45e7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #토크나이저 내부적으로 단어사전과 토크나이저 기능을 corpus에 맞춤형으로 자동 생성하는 함수\n",
    "# def tokenize(corpus):  # corpus: Tokenized Sentence's List\n",
    "#     tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='') # 실험.... \n",
    "#     tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "#     tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "#     tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post') # 실험할 수 있는 부분\n",
    "\n",
    "#     return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7711224b",
   "metadata": {},
   "source": [
    "# filtered corpus 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80449d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 77591\n",
      "문장의 최단 길이: 1\n",
      "문장의 최장 길이: 377\n",
      "문장의 평균 길이: 64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #filtered_corpus 정의\n",
    "# #데이터 가져오기\n",
    "# import os\n",
    "# path_to_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko'\n",
    "# with open(path_to_file, \"r\") as f:\n",
    "#     raw = f.read().splitlines()\n",
    "# #데이터의 중복 제거 \n",
    "# min_len = 999\n",
    "# max_len = 0\n",
    "# sum_len = 0\n",
    "\n",
    "# filtered_corpus = list(set(raw))  # set를 사용해서 중복을 제거합니다.\n",
    "# print(\"Data Size:\", len(filtered_corpus))\n",
    "\n",
    "# for sen in filtered_corpus:\n",
    "#     length = len(sen)\n",
    "#     if min_len > length: min_len = length\n",
    "#     if max_len < length: max_len = length\n",
    "#     sum_len += length\n",
    "\n",
    "# print(\"문장의 최단 길이:\", min_len)\n",
    "# print(\"문장의 최장 길이:\", max_len)\n",
    "# print(\"문장의 평균 길이:\", sum_len // len(filtered_corpus))\n",
    "\n",
    "# sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "# for sen in filtered_corpus:   # 중복이 제거된 코퍼스 기준\n",
    "#     sentence_length[len(sen)-1] += 1\n",
    "\n",
    "# plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "# plt.title(\"Sentence Length Distribution\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287293e3",
   "metadata": {},
   "source": [
    "# SentencePiece 모델 학습 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca20540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sentencepiece as spm\n",
    "# import os\n",
    "\n",
    "# # 1. 준비\n",
    "# corpus_path = os.getenv('HOME') + '/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "# with open(corpus_path, 'w') as f:\n",
    "#     for row in filtered_corpus:\n",
    "#         f.write(str(row) + '\\n')\n",
    "\n",
    "# # 2. 실험할 vocab_size 및 model_type 목록\n",
    "# vocab_sizes = [6000, 8000, 10000]\n",
    "# model_types = ['unigram', 'bpe', 'word', 'char']\n",
    "\n",
    "# # 3. 반복 학습\n",
    "# for vocab_size in vocab_sizes:\n",
    "#     for model_type in model_types:\n",
    "#         model_prefix = f'korean_spm_{model_type}_{vocab_size}'\n",
    "#         print(f\"Training model: {model_prefix}\")\n",
    "#         spm.SentencePieceTrainer.Train(\n",
    "#             f'--input={corpus_path} --model_prefix={model_prefix} '\n",
    "#             f'--vocab_size={vocab_size} --model_type={model_type}'\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37116486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp --model_prefix=korean_spm --vocab_size=8000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 77591 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5080163\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1319\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 77591 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 176815 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 77591\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 241202\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 241202 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=93823 obj=14.8612 num_tokens=530973 num_tokens/piece=5.65931\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=83230 obj=13.5217 num_tokens=533527 num_tokens/piece=6.41027\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=62414 obj=13.5583 num_tokens=554911 num_tokens/piece=8.89081\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=62364 obj=13.5155 num_tokens=555344 num_tokens/piece=8.90488\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=46772 obj=13.6979 num_tokens=583594 num_tokens/piece=12.4774\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46772 obj=13.655 num_tokens=583644 num_tokens/piece=12.4785\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=35079 obj=13.8914 num_tokens=614388 num_tokens/piece=17.5144\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=35079 obj=13.8406 num_tokens=614423 num_tokens/piece=17.5154\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=26309 obj=14.1328 num_tokens=646429 num_tokens/piece=24.5706\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=26309 obj=14.0776 num_tokens=646463 num_tokens/piece=24.5719\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19731 obj=14.4108 num_tokens=680212 num_tokens/piece=34.4743\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19731 obj=14.3488 num_tokens=680249 num_tokens/piece=34.4762\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=14798 obj=14.7201 num_tokens=715259 num_tokens/piece=48.3348\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=14798 obj=14.6487 num_tokens=715261 num_tokens/piece=48.335\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=11098 obj=15.0838 num_tokens=751852 num_tokens/piece=67.7466\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11098 obj=15.0033 num_tokens=751852 num_tokens/piece=67.7466\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8800 obj=15.3986 num_tokens=781580 num_tokens/piece=88.8159\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8800 obj=15.3247 num_tokens=781589 num_tokens/piece=88.8169\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 376710 May  9 07:58 korean_spm.model\r\n",
      "-rw-r--r-- 1 root root 146141 May  9 07:58 korean_spm.vocab\r\n"
     ]
    }
   ],
   "source": [
    "# # SentencePiece 모델 학습 과정\n",
    "# import sentencepiece as spm\n",
    "# import os\n",
    "# temp_file = os.getenv('HOME') + '/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "\n",
    "# vocab_size = 8000   # 실험할 부분\n",
    "\n",
    "# with open(temp_file, 'w') as f:\n",
    "#     for row in filtered_corpus:\n",
    "#         f.write(str(row) + '\\n')\n",
    "        \n",
    "# spm.SentencePieceTrainer.Train(\n",
    "#     '--input={} --model_prefix=korean_spm --vocab_size={}'.format(temp_file, vocab_size)  # 실험할 부분 모델 타입\n",
    "# )\n",
    "\n",
    "# !ls -l korean_spm*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "471acd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1255, 11, 304, 7, 3606, 11, 285, 38, 3]\n",
      "['▁아버지', '가', '방', '에', '들어', '가', '신', '다', '.']\n",
      "아버지가방에들어가신다.\n"
     ]
    }
   ],
   "source": [
    "# #SentencePice 모델 활용\n",
    "# s = spm.SentencePieceProcessor()\n",
    "# s.Load('korean_spm.model')\n",
    "\n",
    "# #SentencePiece를 활용한 sentence -> encoding\n",
    "# tokensIDs = s.EncodeAsIds('아버지가방에들어가신다.')\n",
    "# print(tokensIDs)\n",
    "\n",
    "# #SentencePIece를 활용한 sentence -> encoded pieces\n",
    "# print(s.SampleEncodeAsPieces('아버지가방에들어가신다.', 1, 0.0))\n",
    "\n",
    "# #SentencePiece를 활용한 encoding -> sentence 복원\n",
    "# print(s.DecodeIds(tokensIDs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5bca26",
   "metadata": {},
   "source": [
    "# Tokenizer 함수 작성\n",
    "- SentencePiece를 활용하여 위 함수와 유사한 기능을 하는 sp_tokenize()함수 정의 \n",
    "- 1. 매개변수로 토큰화된 문장의 list를 전달하는 대신 온전한 문장의 list를 전달\n",
    "- 2. 생성된 vocab 파일을 읽어와 {<word> : <idx>} 형태를 가지는 word_index 사전과 {<idx> : <word>} 형태를 가지는 index_word 사전을 생성하고 함께 반환\n",
    "- 3. 리턴값인 tensor는 앞의 함수와 동일하게 토큰화한 후 Encoding된 문장입니다. 바로 학습에 사용할 수 있게 Padding하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e08892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sp_tokenize(s, corpus): \n",
    "\n",
    "#     tensor = []\n",
    "\n",
    "#     for sen in corpus:\n",
    "#         tensor.append(s.EncodeAsIds(sen))\n",
    "\n",
    "#     with open(\"./spm/korean_spm.vocab\", 'r') as f:\n",
    "#         vocab = f.readlines()\n",
    "\n",
    "#     word_index = {}\n",
    "#     index_word = {}\n",
    "\n",
    "#     for idx, line in enumerate(vocab):\n",
    "#         word = line.split(\"\\t\")[0]\n",
    "\n",
    "#         word_index.update({word:idx})\n",
    "#         index_word.update({idx:word})\n",
    "\n",
    "#     tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "#     return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9a0ba6",
   "metadata": {},
   "source": [
    "# 네이버 영화리뷰 감정 분석 문제에 SentencePiece 적용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa312489",
   "metadata": {},
   "source": [
    "- 네이버 영화리뷰 감정 분석 코퍼스에 SentencePiece를 적용시킨 모델 학습하기\n",
    "- 학습된 모델로 sp_tokenize() 메소드 구현\n",
    "- 구현된 토크나이저를 적용하여 네이버 영화리뷰 감정 분석 모델 재학습\n",
    "- KoNLPy 형태소 분석기를 사용한 모델과 성능 비교하기\n",
    "- SentencePiece 모델의 model_type, vocab_size를 변경해 가면서 성능 개선 여부 확인\n",
    "    - vocab_size = 6000, 8000, 10000\n",
    "    - spm.SentencePieceTrainer.Train() : 모델 타입 변경\n",
    "        - unigram(기본값), bpe, word(단어 단위), char(문자 단위)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d72082",
   "metadata": {},
   "source": [
    "## 1. 네이버 영화리뷰 감정 분석 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be56e66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                           document  label\n",
      "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
      "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
      "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
      "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
      "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
      "        id                                           document  label\n",
      "0  6270596                                                굳 ㅋ      1\n",
      "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
      "2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
      "3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
      "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_url = 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt'\n",
    "test_url = 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt'\n",
    "train_data = pd.read_table(train_url)\n",
    "test_data = pd.read_table(test_url)\n",
    "\n",
    "# 확인\n",
    "print(train_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8e5ed18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f694004",
   "metadata": {},
   "source": [
    "## 텍스트만 추출, 공백 제거, 베어있는 문장 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adc7e963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ 중복 제거 전 문장 수: 149995\n",
      "✔️ 중복 제거 후 문장 수: 146182\n"
     ]
    }
   ],
   "source": [
    "# 텍스트만 추출\n",
    "train_corpus = [\n",
    "    str(s).strip()\n",
    "    for s in train_data['document']\n",
    "    if isinstance(s, str) and str(s).strip()\n",
    "]\n",
    "\n",
    "test_corpus = [\n",
    "    str(s).strip()\n",
    "    for s in test_data['document']\n",
    "    if isinstance(s, str) and str(s).strip()\n",
    "]\n",
    "\n",
    "# 중복 제거된 학습 코퍼스\n",
    "filtered_corpus = list(set(train_corpus))\n",
    "\n",
    "print(\"✔️ 중복 제거 전 문장 수:\", len(train_corpus))\n",
    "print(\"✔️ 중복 제거 후 문장 수:\", len(filtered_corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae194d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장의 최단 길이: 1\n",
      "문장의 최장 길이: 146\n",
      "문장의 평균 길이: 35\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 문장 길이 분석\n",
    "#데이터의 중복 제거 \n",
    "min_len = 999\n",
    "max_len = 0\n",
    "sum_len = 0\n",
    "\n",
    "\n",
    "for sen in filtered_corpus:\n",
    "    length = len(sen)\n",
    "    if min_len > length: min_len = length\n",
    "    if max_len < length: max_len = length\n",
    "    sum_len += length\n",
    "\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)\n",
    "print(\"문장의 평균 길이:\", sum_len // len(filtered_corpus))\n",
    "\n",
    "sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "for sen in filtered_corpus:   # 중복이 제거된 코퍼스 기준\n",
    "    sentence_length[len(sen)-1] += 1\n",
    "\n",
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99636d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 길이 5 이상만 사용\n",
    "filtered_corpus = [s for s in filtered_corpus if len(s) >= 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad3b27",
   "metadata": {},
   "source": [
    "max_len = 100 으로 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0700c",
   "metadata": {},
   "source": [
    "## 2. SentencePiece 학습용 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4ba9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def write_temp_file(corpus, save_path):\n",
    "    with open(save_path, 'w') as f:\n",
    "        for line in corpus:\n",
    "            if line.strip():\n",
    "                f.write(line.strip() + '\\n')\n",
    "\n",
    "temp_file = os.path.expanduser('~/sp_corpus.txt')\n",
    "write_temp_file(filtered_corpus, temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc4e9c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /opt/conda/lib/python3.9/site-packages (0.5.2)\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.9/site-packages (from konlpy) (1.21.4)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from konlpy) (1.3.0)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.9/site-packages (from konlpy) (0.4.4)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in /opt/conda/lib/python3.9/site-packages (from konlpy) (4.6.0)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in /opt/conda/lib/python3.9/site-packages (from konlpy) (3.10.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.9/site-packages (from konlpy) (4.6.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /opt/conda/lib/python3.9/site-packages (from tweepy>=3.7.0->konlpy) (2.26.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.9/site-packages (from tweepy>=3.7.0->konlpy) (1.16.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.9/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb33851",
   "metadata": {},
   "source": [
    "## 3. SentencePiece 모델 학습\n",
    "- vocab_size = 8000\n",
    "- model_type = unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79134a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/sp_corpus.txt --model_prefix=spm_unigram_8000 --vocab_size=8000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/sp_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: spm_unigram_8000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/sp_corpus.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 144090 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5397332\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1708\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 144090 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 308664 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 144090\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 356817\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 356817 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=157663 obj=15.4586 num_tokens=841721 num_tokens/piece=5.33874\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=145879 obj=14.3911 num_tokens=847053 num_tokens/piece=5.80655\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=109350 obj=14.4887 num_tokens=882455 num_tokens/piece=8.07\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=109168 obj=14.4328 num_tokens=883223 num_tokens/piece=8.09049\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=81874 obj=14.6692 num_tokens=927183 num_tokens/piece=11.3245\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=81863 obj=14.6077 num_tokens=927367 num_tokens/piece=11.3283\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=61397 obj=14.8676 num_tokens=969179 num_tokens/piece=15.7854\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=61397 obj=14.8069 num_tokens=969242 num_tokens/piece=15.7865\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=46047 obj=15.0975 num_tokens=1014089 num_tokens/piece=22.0229\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46047 obj=15.0331 num_tokens=1014094 num_tokens/piece=22.023\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34535 obj=15.3574 num_tokens=1060395 num_tokens/piece=30.7049\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34535 obj=15.2909 num_tokens=1060452 num_tokens/piece=30.7066\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=25901 obj=15.6489 num_tokens=1108292 num_tokens/piece=42.7895\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25901 obj=15.5763 num_tokens=1108316 num_tokens/piece=42.7905\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19425 obj=15.9624 num_tokens=1158330 num_tokens/piece=59.6309\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19425 obj=15.8822 num_tokens=1158343 num_tokens/piece=59.6316\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=14568 obj=16.3059 num_tokens=1211724 num_tokens/piece=83.1771\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=14568 obj=16.2139 num_tokens=1211730 num_tokens/piece=83.1775\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=10926 obj=16.6826 num_tokens=1268996 num_tokens/piece=116.145\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=10926 obj=16.5715 num_tokens=1269011 num_tokens/piece=116.146\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8800 obj=16.9543 num_tokens=1314006 num_tokens/piece=149.319\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8800 obj=16.8711 num_tokens=1314190 num_tokens/piece=149.34\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: spm_unigram_8000.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: spm_unigram_8000.vocab\n"
     ]
    }
   ],
   "source": [
    "# SentencePiece 모델 학습 과정\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "vocab_size = 8000   \n",
    "\n",
    "with open(temp_file, 'w') as f:\n",
    "    for row in filtered_corpus:\n",
    "        f.write(str(row) + '\\n')\n",
    "        \n",
    "spm.SentencePieceTrainer.Train(\n",
    "    f\"--input={temp_file} --model_prefix=spm_unigram_8000 --vocab_size=8000 --model_type=unigram\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5219d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1838, 8, 5091, 157, 1425, 32, 297, 61, 163, 426, 355, 1342, 7039, 821, 406]\n",
      "['▁흠', '...', '포스터', '보고', '▁초딩', '영화', '줄', '....', '오', '버', '연기', '조차', '▁가볍지', '▁않', '구나']\n",
      "흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n"
     ]
    }
   ],
   "source": [
    "#SentencePice 모델 활용\n",
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('spm_unigram_8000.model')\n",
    "\n",
    "#SentencePiece를 활용한 sentence -> encoding\n",
    "tokensIDs = s.EncodeAsIds('흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나')\n",
    "print(tokensIDs)\n",
    "\n",
    "#SentencePIece를 활용한 sentence -> encoded pieces\n",
    "print(s.SampleEncodeAsPieces('흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', 1, 0.0))\n",
    "\n",
    "#SentencePiece를 활용한 encoding -> sentence 복원\n",
    "print(s.DecodeIds(tokensIDs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b820055",
   "metadata": {},
   "source": [
    "## 4. sp_tokenize() 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d0e0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import sentencepiece as spm\n",
    "\n",
    "def sp_tokenize(model_path, corpus, vocab_path=None, max_len=100):\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(model_path)\n",
    "\n",
    "    # 토큰화 후 시퀀스 생성\n",
    "    tensor = [sp.encode_as_ids(str(s)) for s in corpus]\n",
    "    tensor = pad_sequences(tensor, padding='post', maxlen=max_len)\n",
    "\n",
    "    # vocab 파일 경로 자동 처리\n",
    "    if vocab_path is None:\n",
    "        vocab_path = model_path.replace(\".model\", \".vocab\")\n",
    "\n",
    "    # vocab 파일에서 인덱스 ↔ 서브워드 사전 구성\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            token = line.split('\\t')[0]\n",
    "            word_index[token] = idx\n",
    "            index_word[idx] = token\n",
    "\n",
    "    return tensor, word_index, index_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "493ed7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/sp_corpus.txt --model_prefix=spm_bpe_8000 --vocab_size=8000 --model_type=bpe\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/sp_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: spm_bpe_8000\n",
      "  model_type: BPE\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/sp_corpus.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 144090 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5397332\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1708\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 144090 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 144090\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 356817\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=72674 min_freq=87\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10866 size=20 all=110594 active=10479 piece=▁어\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8818 size=40 all=115166 active=15051 piece=▁1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6278 size=60 all=118639 active=18524 piece=▁생\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5259 size=80 all=122541 active=22426 piece=하다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4193 size=100 all=126334 active=26219 piece=▁볼\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=4154 min_freq=72\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3565 size=120 all=129255 active=9154 piece=▁공\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3201 size=140 all=131832 active=11731 piece=▁좀\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2828 size=160 all=134137 active=14036 piece=보고\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2641 size=180 all=136557 active=16456 piece=입니다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2366 size=200 all=139132 active=19031 piece=▁난\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=2352 min_freq=64\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2209 size=220 all=141351 active=9075 piece=ᄒᄒ\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2036 size=240 all=144628 active=12352 piece=▁하는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1902 size=260 all=147337 active=15061 piece=ᅳᅳ\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1792 size=280 all=149955 active=17679 piece=▁동\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1681 size=300 all=151661 active=19385 piece=OO\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1671 min_freq=58\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1568 size=320 all=153412 active=9325 piece=▁그런\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1443 size=340 all=155641 active=11553 piece=드는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1395 size=360 all=157790 active=13702 piece=▁박\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1322 size=380 all=159875 active=15787 piece=▁누\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1246 size=400 all=162249 active=18161 piece=▁절\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1246 min_freq=53\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1216 size=420 all=163750 active=9571 piece=▁영화다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1168 size=440 all=165570 active=11391 piece=▁ᄒᄒ\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1105 size=460 all=167508 active=13329 piece=▁많은\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1075 size=480 all=169376 active=15197 piece=▁ᄏᄏᄏᄏ\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1020 size=500 all=171166 active=16987 piece=▁욕\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1020 min_freq=49\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=973 size=520 all=172854 active=10183 piece=▁행\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=932 size=540 all=174316 active=11645 piece=▁경\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=902 size=560 all=176259 active=13588 piece=▁독\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=874 size=580 all=178197 active=15526 piece=생각\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=854 size=600 all=180421 active=17750 piece=▁엉\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=854 min_freq=46\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=829 size=620 all=182734 active=11319 piece=엔딩\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=802 size=640 all=184338 active=12923 piece=봤는데\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=776 size=660 all=186478 active=15063 piece=한테\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=755 size=680 all=188076 active=16661 piece=▁긴장감\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=736 size=700 all=189931 active=18516 piece=▁앞\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=736 min_freq=42\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=709 size=720 all=191293 active=10814 piece=이란\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=685 size=740 all=192846 active=12367 piece=▁시작\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=660 size=760 all=194313 active=13834 piece=▁삼\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=648 size=780 all=195449 active=14970 piece=▁싶은\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=634 size=800 all=196872 active=16393 piece=▁작가\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=632 min_freq=40\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=617 size=820 all=198348 active=11245 piece=학교\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=604 size=840 all=199570 active=12467 piece=▁만화\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=592 size=860 all=200798 active=13695 piece=▁황\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=577 size=880 all=202076 active=14973 piece=▁첨\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=563 size=900 all=203063 active=15960 piece=▁토\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=563 min_freq=38\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=553 size=920 all=204369 active=11386 piece=▁시나리오\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=540 size=940 all=205409 active=12426 piece=▁총\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=531 size=960 all=206680 active=13697 piece=▁들어\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=521 size=980 all=207599 active=14616 piece=▁범\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=511 size=1000 all=208665 active=15682 piece=▁얼마나\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=510 min_freq=37\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=504 size=1020 all=210215 active=11966 piece=▁아무리\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=494 size=1040 all=211379 active=13130 piece=▁웃음\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=477 size=1060 all=212757 active=14508 piece=했음\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=468 size=1080 all=214008 active=15759 piece=▁여러\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=457 size=1100 all=215358 active=17109 piece=▁보지마\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=454 min_freq=35\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=443 size=1120 all=216191 active=11580 piece=▁풀\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=436 size=1140 all=217237 active=12626 piece=▁위해\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=425 size=1160 all=218637 active=14026 piece=▁위한\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=417 size=1180 all=220001 active=15390 piece=워서\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=407 size=1200 all=221492 active=16881 piece=▁훨씬\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=407 min_freq=34\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=400 size=1220 all=222381 active=11951 piece=▁있을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=390 size=1240 all=223869 active=13439 piece=년이\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=384 size=1260 all=225377 active=14947 piece=▁재밌고\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=379 size=1280 all=226243 active=15812 piece=▁네이버\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=370 size=1300 all=227572 active=17141 piece=▁s\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=370 min_freq=32\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=363 size=1320 all=228916 active=12704 piece=▁영화관에서\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=357 size=1340 all=229961 active=13749 piece=▁교훈\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=353 size=1360 all=231253 active=15041 piece=▁영화였다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=348 size=1380 all=232112 active=15900 piece=건데\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=343 size=1400 all=232929 active=16717 piece=▁틀\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=343 min_freq=31\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=338 size=1420 all=234089 active=12761 piece=정도로\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=331 size=1440 all=235232 active=13904 piece=▁세계\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=324 size=1460 all=236335 active=15007 piece=영화다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=318 size=1480 all=237768 active=16440 piece=▁뭐지\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=312 size=1500 all=238954 active=17626 piece=▁잃\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=312 min_freq=30\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=308 size=1520 all=239966 active=12935 piece=▁장면이\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=303 size=1540 all=240933 active=13902 piece=▁평범\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=299 size=1560 all=241790 active=14759 piece=▁판타지\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=293 size=1580 all=242626 active=15595 piece=스럽고\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=290 size=1600 all=243496 active=16465 piece=▁자체가\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=289 min_freq=29\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=286 size=1620 all=244286 active=12963 piece=▁탄탄\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=282 size=1640 all=245093 active=13770 piece=남자\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=279 size=1660 all=246071 active=14748 piece=▁관람\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=274 size=1680 all=247217 active=15894 piece=▁꿀잼\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=268 size=1700 all=248199 active=16876 piece=▁됐\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=268 min_freq=28\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=265 size=1720 all=249290 active=13456 piece=▁자극\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=261 size=1740 all=250395 active=14561 piece=▁승\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=259 size=1760 all=251624 active=15790 piece=▁전개가\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=256 size=1780 all=252492 active=16658 piece=▁묻\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=254 size=1800 all=253142 active=17308 piece=▁다시봐도\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=253 min_freq=27\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=249 size=1820 all=253975 active=13484 piece=이냐\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=246 size=1840 all=255267 active=14776 piece=▁80\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=243 size=1860 all=256259 active=15768 piece=▁해야\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=239 size=1880 all=257404 active=16913 piece=▁죽음\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=236 size=1900 all=258087 active=17596 piece=▁법\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=236 min_freq=26\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=233 size=1920 all=258824 active=13600 piece=▁예전에\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=230 size=1940 all=259564 active=14340 piece=명작\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=228 size=1960 all=260699 active=15475 piece=▁연출력\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=226 size=1980 all=261427 active=16203 piece=▁것이다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=223 size=2000 all=262362 active=17138 piece=예산\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=223 min_freq=25\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=220 size=2020 all=263095 active=13815 piece=▁영원\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=217 size=2040 all=263717 active=14437 piece=기엔\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=214 size=2060 all=264524 active=15244 piece=▁촌\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=212 size=2080 all=265192 active=15912 piece=강추\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=210 size=2100 all=266298 active=17017 piece=▁만들지\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=210 min_freq=25\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=207 size=2120 all=267209 active=14218 piece=봅니다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=204 size=2140 all=268004 active=15013 piece=역시\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=203 size=2160 all=268811 active=15820 piece=▁하는데\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=201 size=2180 all=269629 active=16638 piece=▁보여준\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=198 size=2200 all=270332 active=1"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "vocab_sizes = [8000]\n",
    "model_types = ['bpe', 'word', 'char']\n",
    "\n",
    "for vocab_size in vocab_sizes:\n",
    "    for model_type in model_types:\n",
    "        prefix = f'spm_{model_type}_{vocab_size}'\n",
    "        spm.SentencePieceTrainer.Train(\n",
    "            f'--input={temp_file} --model_prefix={prefix} '\n",
    "            f'--vocab_size={vocab_size} --model_type={model_type}'\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cdc0d8",
   "metadata": {},
   "source": [
    "## 3. SentencePiece 기반 RNN 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f5e000",
   "metadata": {},
   "source": [
    "### unigram, vocab_size = 8000\n",
    "- test accuracy : 0.6674"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d350e799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 73s 77ms/step - loss: 0.6955 - accuracy: 0.5012 - val_loss: 0.6948 - val_accuracy: 0.4933\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 72s 76ms/step - loss: 0.6834 - accuracy: 0.5592 - val_loss: 0.6747 - val_accuracy: 0.6001\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.6381 - accuracy: 0.6450 - val_loss: 0.6388 - val_accuracy: 0.6363\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.5996 - accuracy: 0.6755 - val_loss: 0.6095 - val_accuracy: 0.6546\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.5745 - accuracy: 0.6927 - val_loss: 0.5998 - val_accuracy: 0.6701\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.5598 - accuracy: 0.7037 - val_loss: 0.5974 - val_accuracy: 0.6748\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.5469 - accuracy: 0.7142 - val_loss: 0.6014 - val_accuracy: 0.6695\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.5368 - accuracy: 0.7244 - val_loss: 0.6124 - val_accuracy: 0.6631\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.6008 - accuracy: 0.6674\n",
      "✅ SentencePiece + RNN Test Accuracy: 0.6674\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "def build_rnn_model(vocab_size):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, 128),\n",
    "        SimpleRNN(64),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ✅ 올바른 전처리\n",
    "X_train_sp, word_index, index_word = sp_tokenize(\"spm_unigram_8000.model\", train_data['document'])\n",
    "X_test_sp, _, _ = sp_tokenize(\"spm_unigram_8000.model\", test_data['document'])\n",
    "vocab_size = len(word_index)\n",
    "\n",
    "y_train, y_test = train_data['label'].values, test_data['label'].values\n",
    "\n",
    "# ✅ 학습\n",
    "model_sp = build_rnn_model(vocab_size)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "model_sp.fit(X_train_sp, y_train, epochs=20, batch_size=128, validation_split=0.2, callbacks = [early_stop])\n",
    "\n",
    "# ✅ 평가\n",
    "acc_sp = model_sp.evaluate(X_test_sp, y_test)[1]\n",
    "print(f\"✅ SentencePiece + RNN Test Accuracy: {acc_sp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd43ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "vocab_sizes = [8000, 10000]\n",
    "model_types = ['unigram','bpe']\n",
    "\n",
    "for vocab_size in vocab_sizes:\n",
    "    for model_type in model_types:\n",
    "        prefix = f'spm_{model_type}_{vocab_size}'\n",
    "        spm.SentencePieceTrainer.Train(\n",
    "            f'--input={temp_file} --model_prefix={prefix} '\n",
    "            f'--vocab_size={vocab_size} --model_type={model_type}'\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d867a5",
   "metadata": {},
   "source": [
    "### bpe, vocab_size = 8000\n",
    "- test accuracy : 0.7034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "442203f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not found: \"orean_spm_bpe_8000.model\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7085/3931879705.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ✅ 올바른 전처리\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train_sp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"orean_spm_bpe_8000.model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_test_sp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"korean_spm_bpe_8000.model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7085/330627623.py\u001b[0m in \u001b[0;36msp_tokenize\u001b[0;34m(model_path, corpus, vocab_path, max_len)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msp_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# 토큰화 후 시퀀스 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoad\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    365\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromSerializedProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_LoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mDecodeIdsWithCheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Not found: \"orean_spm_bpe_8000.model\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "# ✅ 올바른 전처리\n",
    "X_train_sp, word_index, index_word = sp_tokenize(\"spm_bpe_8000.model\", train_data['document'])\n",
    "X_test_sp, _, _ = sp_tokenize(\"spm_bpe_8000.model\", test_data['document'])\n",
    "vocab_size = len(word_index)\n",
    "\n",
    "y_train, y_test = train_data['label'].values, test_data['label'].values\n",
    "\n",
    "# ✅ 학습\n",
    "model_sp = build_rnn_model(vocab_size)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "model_sp.fit(X_train_sp, y_train, epochs=10, batch_size=128, validation_split=0.2, callbacks = [early_stop])\n",
    "\n",
    "# ✅ 평가\n",
    "acc_sp = model_sp.evaluate(X_test_sp, y_test)[1]\n",
    "print(f\"✅ SentencePiece(bpe, 8000) + RNN Test Accuracy: {acc_sp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6cd664",
   "metadata": {},
   "source": [
    "## bpe, vocabsize=10000\n",
    "- test accuracy = 0.7780"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ec2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 올바른 전처리\n",
    "X_train_sp, word_index, index_word = sp_tokenize(\"spm_bpe_10000.model\", train_data['document'])\n",
    "X_test_sp, _, _ = sp_tokenize(\"spm_bpe_10000.model\", test_data['document'])\n",
    "vocab_size = len(word_index)\n",
    "\n",
    "y_train, y_test = train_data['label'].values, test_data['label'].values\n",
    "\n",
    "# ✅ 학습\n",
    "model_sp = build_rnn_model(vocab_size)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "model_sp.fit(X_train_sp, y_train, epochs=10, batch_size=128, validation_split=0.2, callbacks = [early_stop])\n",
    "\n",
    "# ✅ 평가\n",
    "acc_sp = model_sp.evaluate(X_test_sp, y_test)[1]\n",
    "print(f\"✅ SentencePiece(bpe, 8000) + RNN Test Accuracy: {acc_sp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1475b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sentencepiece as spm\n",
    "# import os\n",
    "# vocab_size = 10000   \n",
    "\n",
    "# with open(temp_file, 'w') as f:\n",
    "#     for row in filtered_corpus:\n",
    "#         f.write(str(row) + '\\n')\n",
    "        \n",
    "# spm.SentencePieceTrainer.Train(\n",
    "#     f\"--input={temp_file} --model_prefix=spm_unigram_10000 --vocab_size=10000 --model_type=unigram\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b46772",
   "metadata": {},
   "source": [
    "### unigram, vocab_size = 10000\n",
    "- test accuracy : 0.8381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9589f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 올바른 전처리\n",
    "X_train_sp, word_index, index_word = sp_tokenize(\"spm_unigram_10000.model\", train_data['document'])\n",
    "X_test_sp, _, _ = sp_tokenize(\"spm_unigram_10000.model\", test_data['document'])\n",
    "vocab_size = len(word_index)\n",
    "\n",
    "y_train, y_test = train_data['label'].values, test_data['label'].values\n",
    "\n",
    "# ✅ 학습\n",
    "model_sp = build_rnn_model(vocab_size)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "model_sp.fit(X_train_sp, y_train, epochs=10, batch_size=128, validation_split=0.2, callbacks = [early_stop])\n",
    "\n",
    "# ✅ 평가\n",
    "acc_sp = model_sp.evaluate(X_test_sp, y_test)[1]\n",
    "print(f\"✅ SentencePiece(unigram, 10000) + RNN Test Accuracy: {acc_sp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e4719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### word, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4923474a",
   "metadata": {},
   "source": [
    "## KoNLPy okt 형태소 분석기를 사용한 전처리 및 모델 학습\n",
    "- test accuracy: 0.8374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc5cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "okt = Okt()\n",
    "def okt_tokenize(text):\n",
    "    text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", str(text))\n",
    "    return okt.morphs(text, stem=True)\n",
    "\n",
    "train_data['okt_tokens'] = train_data['document'].apply(okt_tokenize)\n",
    "test_data['okt_tokens'] = test_data['document'].apply(okt_tokenize)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['okt_tokens'])\n",
    "\n",
    "X_train_okt = pad_sequences(tokenizer.texts_to_sequences(train_data['okt_tokens']), maxlen=100)\n",
    "X_test_okt = pad_sequences(tokenizer.texts_to_sequences(test_data['okt_tokens']), maxlen=100)\n",
    "\n",
    "# model_okt = build_rnn_model(len(tokenizer.word_index)+1)\n",
    "# model_okt.fit(X_train_okt, y_train, epochs=3, batch_size=128, validation_split=0.2)\n",
    "# acc_okt = model_okt.evaluate(X_test_okt, y_test)[1]\n",
    "# print(f\"✅ KoNLPy (Okt) + RNN Test Accuracy: {acc_okt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe9b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_okt = build_rnn_model(len(tokenizer.word_index)+1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "model_okt.fit(X_train_okt, y_train, epochs=10, batch_size=128, validation_split=0.2, callbacks = [early_stop])\n",
    "acc_okt = model_okt.evaluate(X_test_okt, y_test)[1]\n",
    "print(f\"✅ KoNLPy (Okt) + RNN Test Accuracy: {acc_okt:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a2713",
   "metadata": {},
   "source": [
    "## KoNLPy Mecab을 사용한 전처리 및 모델 학습\n",
    "- test accuracy : 0.8422"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7babd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install -y g++ openjdk-8-jdk python3-dev python3-pip curl\n",
    "# !pip install -y konlpy\n",
    "# !curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh | bash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59324fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import re\n",
    "\n",
    "# 1. Mecab 객체 생성\n",
    "mecab = Mecab()\n",
    "\n",
    "# 2. 전처리 함수 정의\n",
    "def mecab_tokenize(text):\n",
    "    text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", str(text))  # 한글만 남기기\n",
    "    return mecab.morphs(text)\n",
    "\n",
    "# 3. 토큰화 적용\n",
    "train_data = train_data.dropna(subset=['document'])\n",
    "test_data = test_data.dropna(subset=['document'])\n",
    "\n",
    "train_data['mecab_tokens'] = train_data['document'].apply(mecab_tokenize)\n",
    "test_data['mecab_tokens'] = test_data['document'].apply(mecab_tokenize)\n",
    "\n",
    "# 4. 토크나이저 학습\n",
    "train_tokens = train_data['mecab_tokens']\n",
    "train_tokens = [tok for tok in train_tokens if isinstance(tok, list) and tok]\n",
    "\n",
    "test_tokens = test_data['mecab_tokens']\n",
    "test_tokens = [tok for tok in test_tokens if isinstance(tok, list) and tok]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_tokens)\n",
    "\n",
    "# 5. 시퀀스 변환 + 패딩\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train_data['mecab_tokens']), maxlen=100)\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(test_data['mecab_tokens']), maxlen=100)\n",
    "y_train = train_data['label'].values\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "# 6. 모델 정의 및 학습\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "model_mecab = Sequential([\n",
    "    Embedding(vocab_size, 128),\n",
    "    SimpleRNN(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_mecab.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "model_mecab.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2, callbacks=[early_stop])\n",
    "\n",
    "# 7. 평가\n",
    "acc_mecab = model_mecab.evaluate(X_test, y_test)[1]\n",
    "print(f\"✅ KoNLPy (Mecab) + RNN Test Accuracy: {acc_mecab:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f5bfc9",
   "metadata": {},
   "source": [
    "## 성능 비교\n",
    "- Mecab이 최고 성능(0.8422)\n",
    "- Ok(0.8374)와 SP-unigram-10k도 거의 동일 수준\n",
    "    - Okt : 0.8374\n",
    "    - SP-unigram-10k : 0.8381\n",
    "- SentencePiece는 vocab_size=10000 이상일 때 성능이 우수해진다. \n",
    "    - unigram + 10000 이 0.8331로 성능 우수\n",
    "    -  SP-bpe는 상대적으로 성능이 낮음  \n",
    "    \n",
    "        bpe-8000: 0.7034  \n",
    "        \n",
    "        bpe-10000: 0.7780  \n",
    "        \n",
    "        → 같은 vocab_size에서도 unigram보다 낮은 성능\n",
    "- vocab_size 증가가 성능 향상에 기여  \n",
    "\n",
    "    SP-bpe: 8000 → 10000 → 정확도 ↑ (0.7034 → 0.7780)  \n",
    "    SP-unigram: 8000 → 10000 → 정확도 ↑ (0.7266 → 0.8381)  \n",
    "    \n",
    "    → 더 많은 서브워드로 문장 표현력 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe69fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 라벨 및 정확도\n",
    "labels = [\n",
    "    \"SP-unigram-8k\", \"SP-bpe-8k\", \"SP-bpe-10k\", \"SP-unigram-10k\",\n",
    "    \"KoNLPy-Okt\", \"KoNLPy-Mecab\"\n",
    "]\n",
    "accuracies = [0.7266, 0.7034, 0.7780, 0.8381, 0.8374, 0.8422]\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(labels, accuracies, color='skyblue')\n",
    "plt.ylim(0.6, 0.9)\n",
    "plt.title(\"Tokenizer accuracy\", fontsize=14)\n",
    "plt.xlabel(\"Tokenizer\", fontsize=12)\n",
    "plt.ylabel(\"Test Accuracy\", fontsize=12)\n",
    "plt.xticks(rotation=15)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# 바 위에 정확도 표시\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.005, f'{acc:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdf6803",
   "metadata": {},
   "source": [
    "# 회고\n",
    "- 같은 모델이라도 사용한 토크나이저에 따라 성능이 크게 달라질 수 있다는 사실을 확인할 수 있었다.\n",
    "- vocab_size 역시 성능에 중요한 영향을 미친다. \n",
    "- SentencePiece는 KoNLPy의 Mecab이나 Okt 처럼 한국어에 특화된 형태소 분석기가 아닌, 언어에 종속되지 않는 범용적 subword 토크나이저\n",
    "- 한국어 특성이 중요한 감정 분석 과제에서 KoNLPy 기반 토크나이저가 성능이 조금 더 높게 나왔다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2109afd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e364c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
